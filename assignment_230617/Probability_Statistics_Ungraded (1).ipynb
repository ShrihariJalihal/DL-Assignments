{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DCgpE0GSFT8a"},"source":["# Advanced Certification Program in Deep Learning\n","## A program by IISc and TalentSprint\n","### Assignment: Probability and Statistics"]},{"cell_type":"markdown","metadata":{"id":"pr0NVbjeFYi5"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"qwcb4wBybvSm"},"source":["At the end of the experiment, you will be able to\n","\n","* understand the concepts of covariance, marginal probability\n","* understand the terms like parameter estimation and bias estimator\n","* understand the concepts of mean squared error, correlation and maximum likelihood estimation\n","*   understand the bayesian inference with prior and posterior distributions"]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"code","metadata":{"id":"xWMVQWk58aXm"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cwqosl928dBA"},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"GXbNUL2L6LoU"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","ipython = get_ipython()\n","\n","notebook= \"Probability_Statistics_Ungraded\" #name of the notebook\n","\n","def setup():\n","    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Customer_Churn.csv\")\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","\n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer1():\n","  try:\n","    if not Answer1:\n","      raise NameError\n","    else:\n","      return Answer1\n","  except NameError:\n","    print (\"Please answer Question 1\")\n","    return None\n","\n","def getAnswer2():\n","  try:\n","    if not Answer2:\n","      raise NameError\n","    else:\n","      return Answer2\n","  except NameError:\n","    print (\"Please answer Question 2\")\n","    return None\n","\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import math\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import scipy\n","import scipy.stats as stats"],"metadata":{"id":"n8Ivgd9bzy7T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RK1KhmWZhRb"},"source":["### Covariance\n","\n","Covariance is a measure of how much two random variables vary together. It is similar to variance.But variance tells you how a single variable varies and covariance tells you how two variables vary together.\n","\n","To calculate covariance we can use the formula as follows:\n","\n","$Cov(x,y) = \\frac{\\sum (x_{i}-x_{m})(y_{i}-y_{m})}{n-1}$"]},{"cell_type":"markdown","metadata":{"id":"RQOEiD10ZhRg"},"source":["1. A company has a data set for five-quarters that represents the following:\n","\n","  a) x = Gross Domestic Product (GDP) growth of each quarter in percent\n","\n","  b) y = Advancement of the company's latest product in percent\n","\n","  The five-quarters dataset for x and y is given below:\n","\n","  x = 2, y = 10\n","\n","  x = 3, y = 14\n","\n","  x = 2.7, y = 12\n","\n","  x = 3.2, y = 15\n","\n","  x = 4.1, y = 20\n","\n","  Find the covariance of x and y using the above dataset?"]},{"cell_type":"code","metadata":{"id":"GqCSmT5yZhRj"},"source":["# Declaring the parameters x and y of the dataset to find the covariance\n","x = [2,3,2.7,3.2,4.1]\n","y = [10,14,12,15,20]\n","\n","# Covariance of x and y using cov()\n","print(\"Variance of x:\",np.cov(x))\n","print(\"Variance of y:\",np.cov(y))\n","print(\"Covariance of x and y:\\n\",np.cov(x,y))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Kl8JaQMZhRl"},"source":["A positive value of covariance shows that new product's growth has a positive relationship with quaterly GDP growth."]},{"cell_type":"markdown","metadata":{"id":"qBZ7dLAqZ4xx"},"source":["### Marginal Probability"]},{"cell_type":"markdown","metadata":{"id":"dBKdMd3oZ4x6"},"source":["The probability of an event that occurs irrespective of the outcome of another event is known as Marginal probability. It is used in the scenarios where we are interested in calculating the probability of an event for a random variable not taking into account the outcome of another random variable.\n","\n","For example, the probability of $X = E_{A}$ for total outcomes of Y.\n","\n","We can calculate the Marginal Probability as follows:\n","\n","$P(X=A) = \\sum  P(X=A, Y=y_{i})$ for all y.\n","\n","This denotes that for a given fixed event $A$, the marginal probability is simply the sum or union of all the probability of all events for the second variable $y$.\n"]},{"cell_type":"markdown","metadata":{"id":"vsTnQcAqZ4yH"},"source":["1. There are three bags colored \"**black**\" \"**white**\" and \"**grey**\"containing \"**green**\" and \"**orange**\" balls in all three of them. The black bag contains 4 orange and 12 green balls. Next, the white bag contains 18 green and 6 orange balls. Further, the grey bag contains 8 orange and 24 green balls. Consider that the probability of picking up the balls from black, white, grey bag are 0.36, 0.28 and 0.44 respectively and you attempted 100 trials. Find the probability (Marginal Probability) of picking up an orange ball.\n","\n","  **Explanation**: The given probability of picking up the balls from the bags are 0.36, 0.28, and 0.44. This means that out of 100 trials, the number of times we will pick the balls from black, white, and grey bags are 36, 28, and 44."]},{"cell_type":"code","metadata":{"id":"WwmF6v2DZ4yX"},"source":["# Number of orange and green balls in black bag\n","o_black = 4\n","g_black = 12\n","\n","# Number of orange and green balls in white bag\n","o_white = 6\n","g_white = 18\n","\n","# Number of orange and green balls in grey bag\n","o_grey = 8\n","g_grey = 24"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VxlxA5OYZ4yi"},"source":["# Given probabilities of picking up the ball from black, white, and grey bags\n","trials = 100\n","p_black = 0.36\n","p_white = 0.28\n","p_grey = 0.44\n","\n","# Out of 100 trials, the number of instances where a ball is picked from black, white and grey bags\n","n_black = p_black*trials\n","n_white = p_white*trials\n","n_grey = p_grey*trials\n","\n","print(n_black)\n","print(n_white)\n","print(n_grey)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RxdSXBrOZ4yq"},"source":["Next, as we want to find the probability of picking up an orange ball, we will first calculate the number of instances where we picked an orange ball from black, white, and grey bags."]},{"cell_type":"code","metadata":{"id":"s6k3gXvxZ4y0"},"source":["ib_orange =  (o_black/(o_black+g_black))*n_black\n","iw_orange = (o_white/(o_white+g_white))*n_white\n","ig_orange = (o_grey/(o_grey+g_grey))*n_grey\n","\n","i_total = ib_orange + iw_orange + ig_orange\n","\n","print(\"No. of instances where we pick orange ball from black bag\\n\",ib_orange)\n","print(\"No. of instances where we pick orange ball from white bag\\n\",iw_orange)\n","print(\"No. of instances where we pick orange ball from grey bag\\n\",ig_orange)\n","print(\"Total instances where we pick orange ball\\n\" ,i_total)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IsQGeFalZ4y6"},"source":["Now, the probability of picking up an orange ball is the sum of probabilities of all the instances where an orange ball is driven out from the bag over the total number of trials."]},{"cell_type":"code","metadata":{"id":"HIl3Xr3aZ4y_"},"source":["marginal_probability = i_total/trials\n","print(marginal_probability)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Customer churn dataset"],"metadata":{"id":"7Hfd4C0ifmFx"}},{"cell_type":"markdown","source":["#### About Data\n","\n","This competition is about predicting whether a customer will change telecommunications provider, something known as \"churning\".\n","\n","The dataset contains 4250 samples. Each sample contains 19 features and 1 boolean variable \"churn\" which indicates the class of the sample. The 19 input features and 1 target variable are:\n","\n","1. \"state\", string: 2-letter code of the US state of customer residence\n","2. \"account_length\", numerical: Number of months the customer has been with the current telecom provider\n","3. \"area_code\", string : \"area_code_AAA\" where AAA = 3 digit area code.\n","4. \"international_plan\", (yes/no): The customer has international plan.\n","5. \"voice_mail_plan\", (yes/no): The customer has voice mail plan.\n","6. \"number_vmail_messages\", numerical: Number of voice-mail messages.\n","7. \"total_day_minutes\", numerical: Total minutes of day calls.\n","8. \"total_day_calls\", numerical: Total minutes of day calls.\n","9. \"total_day_charge\", numerical: Total charge of day calls.\n","10. \"total_eve_minutes\", numerical: Total minutes of evening calls.\n","11. \"total_eve_calls\", numerical: Total number of evening calls.\n","12. \"total_eve_charge\", numerical: Total charge of evening calls.\n","13. \"total_night_minutes\", numerical: Total minutes of night calls.\n","14. \"total_night_calls\", numerical: Total number of night calls.\n","15. \"total_night_charge\", numerical: Total charge of night calls.\n","16. \"total_intl_minutes\", numerical: Total minutes of international calls.\n","17. \"total_intl_calls\", numerical: Total number of international calls.\n","18. \"total_intl_charge\", numerical: Total charge of international calls\n","19. \"number_customer_service_calls\", numerical: Number of calls to customer service\n","20. \"churn\", (yes/no): Customer churn - target variable."],"metadata":{"id":"PTnMF4RaMFAk"}},{"cell_type":"markdown","source":["#### Loading the data"],"metadata":{"id":"46CSQxzhNfbI"}},{"cell_type":"code","source":["customer_churn = pd.read_csv('Customer_Churn.csv')"],"metadata":{"id":"eApOwGcexzlW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["customer_churn.head()"],"metadata":{"id":"LwwFIngnyZy3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Basic data preprocessing"],"metadata":{"id":"OVJtl44CNqn_"}},{"cell_type":"code","source":["# Replacing 'yes' and 'no' values with '1' and '0'.\n","customer_churn['churn'].replace(['yes', 'no'], [1, 0], inplace=True)"],"metadata":{"id":"HETtE6BLdh5u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let us have a look at the columns after replacing churn values with '1' and '0'."],"metadata":{"id":"inABlIH7jDP6"}},{"cell_type":"code","source":["customer_churn.head()"],"metadata":{"id":"JBcSuhE5wZps"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Numerical Features"],"metadata":{"id":"YFoOXgEgBNcp"}},{"cell_type":"code","source":["numerical= customer_churn.select_dtypes(include = 'number').columns\n","\n","categorical = customer_churn.select_dtypes(include = 'object').columns\n","\n","print(f'Numerical Columns:  {customer_churn[numerical].columns}')\n","print('\\n')\n","print(f'Categorical Columns: {customer_churn[categorical].columns}')"],"metadata":{"id":"Nx45gEOpS5M4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For ease of usage, we got the list of the numerical and categorical features."],"metadata":{"id":"htFLrswPTxgb"}},{"cell_type":"code","source":["num_col = []\n","\n","# Checking for the numnerical columns\n","for col in numerical:\n","    if customer_churn[col].dtype == \"int64\":\n","        num_col.append(col)\n","\n","num_col.remove(\"churn\")\n","print(num_col)"],"metadata":{"id":"3hWt8rYbBPwc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert all types columns (except for churn) with int64 into float type.\n","for i in num_col:\n","  customer_churn[i] = customer_churn[i].astype(float)"],"metadata":{"id":"DriZ4xaiTjup"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Correlation**\n","\n","The correlation is a statistical measure of the strength of the relationship between the relative movements of two variables. We use correlation coefficient to measure the strength of relationship. The values range between -1.0 and 1.0. A calculated number greater than 1.0 or less than -1.0 means that there was an error in the correlation measurement. A correlation of -1.0 shows a perfect negative correlation, while a correlation of 1.0 shows a perfect positive correlation. A correlation of 0.0 shows no linear relationship between the movement of the two variables.\n","\n","**Note:** The correlation we are referring here is Pearson correlation.\n","\n","Some key points related to correlation are:\n","\n","1. Correlation has no units.\n","2. A key mathematical property of the Pearson correlation coefficient is that it is invariant under separate changes in location and scale in the two variables. That is, we may transform $X$ to $a + bX$ and transform $Y$ to $c + dY$, where $a, b, c$ and $d$ are constants with $b, d > 0$, without changing the correlation coefficient.\n","An important limitation of the correlation coefficient is that it assumes a linear association.\n","3. Correlation between two random variables, $\\rho (X,Y)$ is the covariance of the two variables normalized by the variance of each variable. This normalization cancels the units out and normalizes the measure so that it is always in the range [0, 1]:\n","\n","  $\\rho (X, Y) = \\frac{Cov(X, Y)}{\\sqrt{(Var(X)Var(Y))}}$\n","\n","**Example** : Correlation statistics can be used in finance and investing. A correlation coefficient could be calculated to determine the level of correlation between the price of crude oil and the stock price of an oil-producing company, such as Exxon Mobil Corporation. Since oil companies earn greater profits as oil prices rise, the correlation between the two variables is highly positive.\n"],"metadata":{"id":"siXc6xc6LhvT"}},{"cell_type":"markdown","source":["2. Find the features in the customer churn dataset which has the highest correlation."],"metadata":{"id":"5P2uOZqIZCfF"}},{"cell_type":"code","source":["# Function for correlation plot\n","def correlation_plot():\n","  plt.figure(figsize=(12, 6))\n","  sns.heatmap(customer_churn[numerical].corr(), annot=True, fmt= '.2f', vmin=-1, vmax=1, center=0, cmap='coolwarm');"],"metadata":{"id":"pYAfRWG-UX1_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["correlation_plot()"],"metadata":{"id":"2lSn7Ilid5mW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* We can see in heatmap that we have some multicollinerity.\n","* We need to drop one of each highly correleated column pairs."],"metadata":{"id":"0LhlHFX2U1If"}},{"cell_type":"code","source":["# Dropping the columns\n","drop_col = ['total_day_charge', 'total_eve_charge', 'total_night_charge', 'total_intl_charge']\n","customer_churn = customer_churn.drop(drop_col, axis=1)\n","customer_churn.shape"],"metadata":{"id":"Hm-Z5DjJVGFI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Selecting the numerical columns\n","numerical = customer_churn.select_dtypes(include = 'number').columns\n","print(numerical)"],"metadata":{"id":"SqbAdzxmWH0x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["correlation_plot()"],"metadata":{"id":"Whcdtfr8V5Nz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, We got rid of multicollinear columns. Here, we can see that `total_day_minutes` has the highest correleation with `churn`.Overall, there is low correleations among features."],"metadata":{"id":"TY_tNhE_YPJ4"}},{"cell_type":"markdown","source":["\n","**Standard Deviation**\n","\n"],"metadata":{"id":"iBYUSckCLwFT"}},{"cell_type":"markdown","source":["3. Find the standard deviation of the feature which is highly correlated with `churn` in the dataset."],"metadata":{"id":"c0NlCZQeb8es"}},{"cell_type":"code","source":["np.std(customer_churn['total_day_minutes'])"],"metadata":{"id":"XO2BhiKeaQrw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZuzPSz6nKazP"},"source":["4. Calculate the variance of the feature which has the second highest correlation with churn.\n"]},{"cell_type":"code","source":["customer_churn.total_eve_minutes.var()"],"metadata":{"id":"HVv4tNcEO2r1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`Consider another data of employees salaries, number of working hours, and experience in years and answer the below question.`\n","\n","`dataframe = (`\n","\n","`  {'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],`\n","\n","  `'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000],`\n","\n","  `'Hours':[41,40,36,17,35,39,40],`\n","\n","  `'Experience(In Years)':[2, 2, 3, 7, 5, 2, 4]})`\n","\n","\n","\n","\n"],"metadata":{"id":"Aksyz_VjPfzi"}},{"cell_type":"markdown","metadata":{"id":"4OMitShRMLp5"},"source":["5. Calculate the correlation between Salary and Experience(In Years) columns for the employees data given above.\n"]},{"cell_type":"code","metadata":{"id":"mLRHnAdSGLj4"},"source":["# Create a dataframe\n","df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n","                   'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000],\n","                   'Hours':[41, 40, 36, 17, 35, 39, 40],\n","                   'Experience(In Years)':[2, 2, 3, 7, 5, 2, 4]})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AqBloFehMU42"},"source":["# Calculate the correlation between Salary and Experience(In Years)\n","print(df['Experience(In Years)'].corr(df['Salary']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H5s-vNYRM115"},"source":["6. Find the quartile thresholds for the weekly hours worked by the employees for the above employees data."]},{"cell_type":"code","metadata":{"id":"fIJnRxjNNIvk"},"source":["print(\"The quartile threshold values are\\n\", df['Hours'].quantile([0.25, 0.5, 0.75]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W4N56ItzNVE1"},"source":["### Parameter Estimation"]},{"cell_type":"markdown","metadata":{"id":"ps5EQE0TNXQm"},"source":["**Parameter Estimation:** Parameters are defined as the characteristics of the given data. Estimators are defined as the predictions of values with the help of the dataset.\n","\n","Any function of a random sample that is used to estimate the values of the parameter of a given dataset is called Parameter Estimation.\n","\n","If $x_1,x_2,x_3,....x_n$ is the sample size of size n, then\n","\n","$T_n(x_1,x_2,x_3,....x_n)$ will be the estimator of parameter."]},{"cell_type":"markdown","metadata":{"id":"FDwUsYHnNgOw"},"source":["### Bias Estimation"]},{"cell_type":"markdown","metadata":{"id":"TnTNYElJNiMU"},"source":["**Bias Estimation:**  Bias is a term that refers to any type of error or distortion that is found with the use of analysis. Bias Estimation is the difference between the parameter to be estimated and the mathematical expectation of the estimator.\n","\n","$bias(\\hatθ_n)$ = $E[\\hatθ_n]- \\hatθ_n$\n","\n","The estimator is said to be unbiased when expected parameter and the original parameter are same i.e $E[θ_n^|] = θ_n^|.$"]},{"cell_type":"markdown","metadata":{"id":"WjXvsuicNkjC"},"source":["`7.` `Calculate the bias for the below given dataframe.`\n","\n","\n","  `dataframe = {`\n","\n","  `'Expected_parameter':[2,3,1,5,6,7,8,9,6,5,4,3,2],`\n","\n","  `'Original_parameter':[3,4,3,2,2,4,5,6,9,6,6,4,5]`\n","`}`\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"xzterEgxNoii"},"source":["data = {\n","    'Expected_parameter':[2,3,1,5,6,7,8,9,6,5,4,3,2],\n","    'Original_parameter':[3,4,3,2,2,4,5,6,9,6,6,4,5]\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataframe = pd.DataFrame(data)\n","dataframe['Bias'] = dataframe['Expected_parameter'] - dataframe[\"Original_parameter\"] # Calculating the bias according to the formula\n","print(\"The bias is\\n\", dataframe[\"Bias\"])"],"metadata":{"id":"lFSV2AN7s98Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7z5CKXI3NzeI"},"source":["### Mean Squared Error"]},{"cell_type":"markdown","metadata":{"id":"zoqp2f84Nnz6"},"source":["**Mean Squared Error :**\n","\n","\n","Let $\\hat{X}=g(Y)$ be an estimator of the random variable $X$, given that we have observed the random variable $Y$. The mean squared error (MSE) of this estimator is defined as\n","\n","$E[(X−\\hat{X})^2]=E[(X−g(Y))^2].$\n"]},{"cell_type":"markdown","source":["8. Consider two estimators $E_{1}$  and $E_{2}$ used by two different Machine Learning algorithms.These estimators are used to predict the values using the dataset. If the true values present in the dataset for the predictor variable are [ 1, 1, 2, 2, 4 ] and the values predicted by the two estimators are: $E_{1}$ = [ 0.6, 1.29, 1.99, 2.69, 3.4 ], and $E_{2}$ = [0.45, 1.19 ,1.99 ,1.69, 2.4]. Find out which estimator gives better estimate for prediction."],"metadata":{"id":"QkLuWMemTpqh"}},{"cell_type":"code","metadata":{"id":"5KiW-bNsN9_0"},"source":["# Defining Y_true and Y_pred values\n","Y_true = [1,1,2,2,4]\n","E1_pred = [0.6,1.29,1.99,2.69,3.4]\n","E2_pred = [0.45, 1.19 ,1.99 ,1.69, 2.4]\n","\n","# Calculating Mean Squared Error\n","MSE1 = np.square(np.subtract(Y_true,E1_pred)).mean()\n","MSE2 = np.square(np.subtract(Y_true,E2_pred)).mean()\n","\n","print(\"The mean Square Error for Estimator 1 is \",MSE1)\n","print(\"The mean Square Error for Estimator 2 is \",MSE2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l6ZMBUuEN_zf"},"source":["### Maximum Likelihood Estimation"]},{"cell_type":"markdown","metadata":{"id":"VjGSs8O9OBwc"},"source":["**Maximum Likelihood Estimation:** Maximum likelihood estimation is a method that determines maximum values for the parameter of a model."]},{"cell_type":"markdown","metadata":{"id":"ZZz5fFwOOEdp"},"source":["9. Build 1000 data points from the normal distribution with  mean = 1 and standard deviation = 5. Estimate their maximum likelihood of mean, variance, and standard deviation from the data."]},{"cell_type":"code","metadata":{"id":"WNLcwPvYOGdS"},"source":["# Building the distributed data from mean and standard deviation\n","mean = 1\n","std = 5\n","N_points = 1000\n","\n","#  Finding the random normal distributed data\n","data = np.random.normal(mean, std, N_points)\n","sns.histplot(data, kde=True, color=\"red\", stat=\"density\", linewidth=0);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Inp5af9gOIau"},"source":["# Printing the original mean and variance\n","print('Original mean is '+str(mean) + ', variance is ' + str(std**2),\" and standard deviation is \" +str(std))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JAxlMkzvONuV"},"source":["# Calculating the mean from the data\n","mu_ML = 1/N_points * sum(data)\n","print('Maximum likelihood estimation of mean from the normal distributed data is '+str(mu_ML))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"frylF700OPSA"},"source":["# Calculating the variance from the data\n","var_ML = (1/(N_points-1)) * sum([(x-mu_ML) ** 2 for x in data])\n","print('Maximum likelihood estimation of variance from the normal distributed data is ' + str(var_ML))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CyJ8_Yi6ORds"},"source":["# Calculating the standard deviation from the data\n","std_ML = math.sqrt(var_ML)\n","print(\"Maximum likelihood estimation of standard deviation from the normal distributed data is  \"+str(std_ML))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oBGn2dPCapoZ"},"source":["### Bayesian inference"]},{"cell_type":"markdown","metadata":{"id":"jLsEzKdcar0Q"},"source":["**Bayesian inference :**- Bayesian inference is a method of statistical inference in which Baye's theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics.\n","\n","The core of Bayesian Inference is to combine two different distributions (likelihood and prior) into one “smarter” distribution (posterior). Posterior is **“smarter” in the sense that the classic maximum likelihood estimation (MLE) doesn’t take into account a prior**. Once we calculate the posterior, we use it to find the “best” parameters and the **“best” is in terms of maximizing the posterior probability**, given the data. This process is called Maximum A Posteriori (MAP).\n","\n","Bayesian Inference has three steps.\n","\n","Step 1. **[Prior] Choose a PDF to model your parameter θ**, aka the prior distribution **P(θ)**. This is **your best guess** about parameters **before** seeing the data **X**.\n","\n","Step 2. **[Likelihood] Choose a PDF for $P(X|θ)$**. Basically you are modeling how the data **$X$** will look like given the parameter **$θ$**.\n","\n","Step 3. **[Posterior] Calculate the posterior** distribution **$P(θ|X)$** and pick the **$θ$ that has the highest $P(θ|X)$**.\n","\n","And the posterior becomes the new prior. Repeat step 3 as you get more data.\n","\n","\n","**Formula for calculating Posterior**\n","\n","$P(θ|X) = \\frac{P(X|θ).P(θ)}{\\int P(X|θ).P(θ).dθ}$\n","\n","where, $P(θ|X)$ = Posterior, $P(X|θ)$ = Sampling, $P(θ)$ = Prior, $dθ$ = Normalizing constant\n","\n","**Note**: We will follow these steps to work on 11th question."]},{"cell_type":"markdown","metadata":{"id":"LeqpePDLaveP"},"source":["10. Bob is selecting one marble from two bowls of marbles. The first bowl has 75 red marbles and 25 blue marbles. The second bowl has 50 red marbles and 50 blue marbles. Given that Bob is equally likely to choose from either bowl and does not discriminate between the marbles themselves, Bob in fact chooses a red marble. What is the probability Bob picked the marble from bowl 1 and bowl 2?"]},{"cell_type":"code","metadata":{"id":"g6-fP5raayPb"},"source":["# Probability of fetching the marble from bowl\n","P_H_1 = P_H_2 = 0.5\n","\n","# Probability of fetching the red marble from first bowl\n","P_E_H_1 = 75/100\n","\n","# Probability of fetching the marble from second bowl\n","P_E_H_2 = 50/100\n","\n","# Applying the Bayesian formula to pick the red marble from first bowl\n","P_H_1_E = (P_E_H_1*P_H_1)/((P_E_H_1*P_H_1)+(P_E_H_2*P_H_2))\n","P_H_2_E = (P_E_H_2*P_H_2)/((P_E_H_1*P_H_1)+(P_E_H_2*P_H_2))\n","print(\"The probability of fetching the red marble from bowl 1 is \"+str(P_H_1_E)+ \" and from bowl 2 is \"+str(P_H_2_E))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bTCNpvaZa16i"},"source":["### Prior and Posterior distribution"]},{"cell_type":"markdown","metadata":{"id":"I-2iTwz8a57C"},"source":["**Prior distribution:**\n","A Prior distribution represents your belief about the true or false value of a parameter. It shows your “best guess.”"]},{"cell_type":"markdown","metadata":{"id":"ZvgASXe4a77D"},"source":["**Posterior distribution:**  The posterior distribution summarizes what you know after the data has been observed. The summary of the evidence from the new observations is the likelihood function.\n","It is represented as $Posterior$ $Distribution$ $=$ $Prior$ $Distribution * Likelihood$ $Function (“new$ $evidence”)$"]},{"cell_type":"markdown","source":["11. Consider a scenario, where, an education management company conducts free sessions on YouTube on various topics related to Deep Learning. Everytime a session is conducted, around 4000 people actively attends it. After the experts closes the session, a feedback is asked from the attendees. Some people liked the session and some don't. Now, we would like to make predictions about what percentage of people will engage and like when we conduct a session in the future, so that the company can understand about the potential participants willing to continue learning by enrolling in their professional courses.\n","\n"],"metadata":{"id":"yRLt6LchmNRz"}},{"cell_type":"markdown","source":["Let's generate the data X."],"metadata":{"id":"GgJFH9rznfm4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BP3hxvzLsebe"},"outputs":[],"source":["np.set_printoptions(threshold=100)\n","\n","# Generating 4,000 participants reponse.\n","# Assuming the 'likes' or 'dislikes' follow a Bernoulli process - a sequence of binary (success/failure) random variables.\n","# 1 means liked. 0 means dislike.\n","\n","# We pick the success rate of 30%.\n","liked_prob = 0.3\n","\n","# IID (independent and identically distributed) assumption\n","liked_data = np.random.binomial(n=1, p=liked_prob, size=4000)"]},{"cell_type":"markdown","source":["Let us have a look at liked data."],"metadata":{"id":"9Iu5QZzVogbk"}},{"cell_type":"code","source":["print(liked_data)"],"metadata":{"id":"alGLsVtd1nkz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(liked_data)"],"metadata":{"id":"gES95aP_1sGs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Firstly, we will choose the PDF to model the parameter $\\theta$.\n","\n","Note : $\\theta$ is the **'liking'** probability.\n"],"metadata":{"id":"5pxBSfnBqQzL"}},{"cell_type":"markdown","source":["`i) What kind of probability distributions should we use to model a probability?`\n","\n","**Explaination**: Since, we have only one thing to predict, we will use a Beta distribution. It has two parameters, $α$ & $β$, that we need to decide. You can think of $α$ as How many people liked (the number of successes) and $β$ as how many people did’t liked (the number of failures). These parameters — how big or small $α$ & $β$ are — will determine the shape of the distribution."],"metadata":{"id":"KGtDV0WYvWk2"}},{"cell_type":"markdown","source":["`ii) Let us assume that we have 800 people out of 4000 who liked the session. Write this in terms of beta distribution and plot the prior distribution with respect to all` $\\theta$ `values`."],"metadata":{"id":"TgMS9sfGvufx"}},{"cell_type":"code","source":["# Declaring alpha and beta\n","alpha = 800\n","beta = 4000 - alpha\n","\n","# domain θ\n","theta_range = np.linspace(0, 1, 2000)\n","\n","# prior distribution P(θ)\n","prior = stats.beta.pdf(x = theta_range, a=alpha, b=beta)"],"metadata":{"id":"kkRa4KY21xA_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotting the distribution"],"metadata":{"id":"ZYtL7DFkzTCz"}},{"cell_type":"code","source":["# Plotting the prior distribution\n","plt.rcParams['figure.figsize'] = [20, 7]\n","fig, ax = plt.subplots()\n","plt.plot(theta_range, prior, linewidth=3, color='palegreen')\n","\n","# Add a title\n","plt.title('[Prior] PDF of \"Probability of Claps\"', fontsize=20)\n","\n","# Add X and y Label\n","plt.xlabel('θ', fontsize=16)\n","plt.ylabel('Density', fontsize=16)\n","\n","# Add a grid\n","plt.grid(alpha=.4, linestyle='--')\n","\n","# Show the plot\n","plt.show()"],"metadata":{"id":"hIA9sLIz10Ut"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can observe from the above plot that it spikes at 20% (800 likes / 4000 participants) as expected."],"metadata":{"id":"mTj_2qES0f-m"}},{"cell_type":"markdown","source":["Further, lets get into step 2 which is Likelihood $P(X|θ)$.\n","\n","Choose a probability model for $P(X|θ)$, the probability of seeing the data $X$ given a particular parameter $θ$. We can also call Likelihood as a sampling distribution."],"metadata":{"id":"fpobrWWr1pqC"}},{"cell_type":"markdown","source":["`iii) Find out which probability distribution should be used to model the sampling distribution and the likelihood?`\n","\n","\n"],"metadata":{"id":"8fZtoqoK68eq"}},{"cell_type":"markdown","source":["**Hint:** Since $X$ is binary, and we also have the total number of participants $(n)$ and we want the probability of liked $(p)$. So, we can use Binomial Distribution with $n$ and $p$."],"metadata":{"id":"L7b4_6F_3VC4"}},{"cell_type":"code","source":["# The sampling dist P(X|θ) with a prior θ\n","likelihood = stats.binom.pmf(k = np.sum(liked_data), n = len(liked_data), p = alpha/(alpha+beta))\n","print(likelihood)"],"metadata":{"id":"htP_gOW_15NW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`iv)` `Plot the graph for` $P(X|θ)$ `for all possible` $θ$.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"m17Tg1uV4BqI"}},{"cell_type":"code","source":["# Likelihood P(X|θ) for all θ's\n","likelihood = stats.binom.pmf(k = np.sum(liked_data), n = len(liked_data), p = theta_range)\n","\n","# Create the plot\n","fig, ax = plt.subplots()\n","plt.plot(theta_range, likelihood, linewidth=3, color='yellowgreen')\n","\n","# Add a title\n","plt.title('[Likelihood] Probability of people who liked the session' , fontsize=20)\n","\n","# Add X and y Label\n","plt.xlabel('θ', fontsize=16)\n","plt.ylabel('Probability', fontsize=16)\n","\n","# Add a grid\n","plt.grid(alpha=.4, linestyle='--')\n","\n","# Show the plot\n","plt.show()"],"metadata":{"id":"CGkQt0Fc2D9c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we will try to calculate the posterior distribution.\n","\n","Even though there are thousands of data points, we can convert them into a single scalar — the likelihood **$P(X|θ)$ — by plugging data into the model that you choose** (in this example, the binomial distribution).\n","\n","Then, we calculate **$P(θ)$ & $P(X|θ)$** for a specific **$θ$** and multiply them together. If you do this for every possible **$θ$**, you can pick the highest **$P(θ)$ * $P(X|θ)$** among different **$θ’s$**.\n","\n","Your initial guess about parameters was **$P(θ)$**. Now you are **upgrading a simple $P(θ)$ into something more informative — $P(θ|X)$ — as more data become available.**\n","**$P(θ|X)$** is still the probability of **$θ$**, just like **$P(θ)$** is. However, **$P(θ|X)$** is a smarter version of **$P(θ)$**."],"metadata":{"id":"OebbkziVDnMM"}},{"cell_type":"markdown","source":["`v). Calculate the Posterior Distribution` $P(θ|X)$?"],"metadata":{"id":"k2CyayTtH_GV"}},{"cell_type":"code","source":["# Finding the Prior\n","theta_range_e = theta_range + 0.001\n","prior = stats.beta.cdf(x = theta_range_e, a=alpha, b=beta) - stats.beta.cdf(x = theta_range, a=alpha, b=beta)\n","# prior = stats.beta.pdf(x = theta_range, a=alpha, b=beta)\n","\n","# Finding likelihood\n","likelihood = stats.binom.pmf(k = np.sum(liked_data), n = len(liked_data), p = theta_range)\n","\n","# Element-wise multiplication\n","posterior = likelihood * prior\n","normalized_posterior = posterior / np.sum(posterior)"],"metadata":{"id":"UdWj32-135ep"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** We calculated the prior by subtracting two `stats.beta.cdf` instead of using `stats.beta.pdf` because the likelihood `stats.binom.pmf` is a probability while `stats.beta.pdf` returns a density. Even if we use the density to calculate the posterior, it won’t change the optimization result. However, if you want the units to match, converting a density into a probability is necessary."],"metadata":{"id":"E6jczph5I3gs"}},{"cell_type":"markdown","source":["`vi). Plot the graph for Prior, Likelihood, and Posterior together.`"],"metadata":{"id":"yjbzrjg6HL2u"}},{"cell_type":"code","source":["# Plotting all three together\n","fig, axes = plt.subplots(3, 1, sharex=True, figsize=(20,7))\n","plt.xlabel('θ', fontsize=24)\n","axes[0].plot(theta_range, prior, label=\"Prior\", linewidth=3, color='palegreen')\n","axes[0].set_title(\"Prior\", fontsize=16)\n","axes[1].plot(theta_range, likelihood, label=\"Likelihood\", linewidth=3, color='yellowgreen')\n","axes[1].set_title(\"Sampling (Likelihood)\", fontsize=16)\n","axes[2].plot(theta_range, posterior, label='Posterior', linewidth=3, color='olivedrab')\n","axes[2].set_title(\"Posterior\", fontsize=16)\n","plt.show()"],"metadata":{"id":"wsr9LhoP4F4a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When you look at the posterior graph (the 3rd one), **notice it is where the likelihood shifted toward the prior**. The **liked** probability for the prior was 20%. The **liked** probability for the data was given as 30%. Now, the posterior has its peak around 0.25%.\n","\n","Also, notice the width of the bell curves in prior/likelihood has shrunk in the posterior. Because we incorporated more information through sampling, the range of possible parameters is now narrower.\n","\n","The more data you gather, the graph of the posterior will look more like that of the likelihood and less like that of the prior. In other words, as you get more data, the original prior distribution matters less.\n","\n","Finally, we can pick **$\\theta$ that gives the highest posterior** computed by numerical optimization, such as the Gradient Descent or newton method. This whole iterative procedure is called **Maximum A Posteriori estimation (MAP)**."],"metadata":{"id":"hn_DYtKLJOBh"}},{"cell_type":"markdown","metadata":{"id":"Q0vkPqqia-RK"},"source":["12. Construct a prior and posterior distribution with some binominal random distribution techniques."]},{"cell_type":"code","metadata":{"id":"8DQzNpiRbBXD"},"source":["def bern_post(n_params=1000, n_sample=1000, true_p=.5, prior_p=.5, n_prior=1000):\n","    # Creating the samples\n","    params = np.linspace(0, 1, n_params)\n","    sample = np.random.binomial(n=1, p=true_p, size=n_sample)\n","\n","    # Calculating the Likelihood\n","    likelihood = np.array([np.product(stats.bernoulli.pmf(sample, p)) for p in params])\n","    likelihood = likelihood / np.sum(likelihood)\n","\n","    # Prior sample\n","    prior_sample = np.random.binomial(n=1, p=prior_p, size=n_prior)\n","    prior = np.array([np.product(stats.bernoulli.pmf(prior_sample, p)) for p in params])\n","    prior = prior / np.sum(prior)\n","\n","    # Finding the posterior\n","    posterior = [prior[i] * likelihood[i] for i in range(prior.shape[0])]\n","    posterior = posterior / np.sum(posterior)\n","\n","    # Plotting the graph\n","    fig, axes = plt.subplots(3, 1, sharex=True, figsize=(8,8))\n","    axes[0].plot(params, likelihood)\n","    axes[0].set_title(\"Sampling Distribution\")\n","    axes[1].plot(params, prior)\n","    axes[1].set_title(\"Prior Distribution\")\n","    axes[2].plot(params, posterior)\n","    axes[2].set_title(\"Posterior Distribution\")\n","    sns.despine()\n","    plt.tight_layout()\n","\n","    return posterior"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9eRUzYobFcP"},"source":["moredata_post = bern_post(n_sample=1000)"],"execution_count":null,"outputs":[]}]}