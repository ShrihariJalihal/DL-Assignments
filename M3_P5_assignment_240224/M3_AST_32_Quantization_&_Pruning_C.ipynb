{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Nwm4FK3wgU"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment : IoT and Edge Devices - Quantization and Pruning of Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu26Vq9jDTpj"
      },
      "source": [
        "### Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "*  understand about quantization\n",
        "*  batchnorm folding\n",
        "*  quantization aware training\n",
        "*  understand role of pruning in minimization of the resource(power, memory, number of computations) requirements at test time\n",
        "*  implement iterative pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NiLPMhPgYFa"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9CxCComgihL"
      },
      "source": [
        "#### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0k4a9Qbe6p1"
      },
      "source": [
        "In this experiment, we will use the CIFAR-10 dataset from keras API. It consists of 60,000 colour images(32x32) in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images.\n",
        "\n",
        "\n",
        "\n",
        "Here are the classes in the dataset, as well as 10 random images from each:\n",
        "\n",
        "\n",
        "<img src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Images/CIFAR10.png\" alt=\"Drawing\" height=\"350\" width=\"440\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "Deep learning has a growing history of successes, but heavy algorithms running on large graphical processing units are far from ideal. A relatively new family of deep learning methods called quantized neural networks have appeared in answer to this discrepancy. Quantization methods helps enabling efficient high-performance deep learning computation on small devices.\n",
        "\n",
        "Moreover, Deep learning for classification tasks involves training the parameters of a neural network such that the algorithm learns to discern between object classes. This is achieved by feeding many images of labelled data to the neural network, while updating the parameters to increase performance on a smooth objective function. A drawback is that a large number of parameters are used, compared to more traditional algorithms.\n",
        "\n",
        "Thus enters quantization as a method to bring the neural network to a reasonable size, while also achieving high performance accuracy. This is especially important for on-device applications, where the memory size and number of computations are necessarily limited. Quantization for deep learning is the process of approximating a neural network that uses floating-point numbers by a neural network of low bit width numbers. This dramatically reduces both the memory requirement and computational cost of using neural networks.\n",
        "\n",
        "\n",
        "**Note:** Refer to the following to understand more about [Quantization](https://pytorch.org/docs/stable/quantization.html)"
      ],
      "metadata": {
        "id": "TFNIzE-djhjR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2240589\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9886499911\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "rWzUR5IMqb7b",
        "outputId": "08bba022-de42-4c51-e418-97e2f4a1b14e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2240589&recordId=2636\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M3_AST_32_Quantization_&_Pruning_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required packages"
      ],
      "metadata": {
        "id": "pNYuAjidjlGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from typing import Type, Any, Callable, Union, List, Optional\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LuznkOLEjwAO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining helper functions to download and load the data"
      ],
      "metadata": {
        "id": "11swMCS8j5V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_random_seeds(random_seed=0):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "\n",
        "def prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256):\n",
        "\n",
        "    train_transform = transforms.Compose([   # Define transformations for train and test sets\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    # Loading train and test dataset\n",
        "    train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transform)\n",
        "\n",
        "    test_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "    # A Sampler that randomly shuffled indices\n",
        "    # A RandomSampler with a size and dtype for the stored indices.\n",
        "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
        "\n",
        "    # A Sampler that returns indices sequentially\n",
        "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
        "\n",
        "    # Loading the train and test dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_set, batch_size=train_batch_size,\n",
        "        sampler=train_sampler, num_workers=num_workers)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        dataset=test_set, batch_size=eval_batch_size,\n",
        "        sampler=test_sampler, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "zdcj6vRVvS8r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization Code for Model Training"
      ],
      "metadata": {
        "id": "qdJu24TpjqFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source code for torchvision.models.resnet"
      ],
      "metadata": {
        "id": "E7dVMz_YyOLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
        "           'wide_resnet50_2', 'wide_resnet101_2']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}"
      ],
      "metadata": {
        "id": "A7kMsVo8x00k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F8sXiKScEmcZ"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        # Rename relu to relu1\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "        # Remember to use two independent ReLU for layer fusion.\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # Use FloatFunctional for addition for quantization compatibility\n",
        "        # out += identity\n",
        "        out = self.skip_add.add(identity, out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion: int = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # out += identity\n",
        "        out = self.skip_add.add(identity, out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        layers: List[int],\n",
        "        num_classes: int = 1000,\n",
        "        zero_init_residual: bool = False,\n",
        "        groups: int = 1,\n",
        "        width_per_group: int = 64,\n",
        "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # Each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
        "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(\n",
        "    arch: str,\n",
        "    block: Type[Union[BasicBlock, Bottleneck]],\n",
        "    layers: List[int],\n",
        "    pretrained: bool,\n",
        "    progress: bool,\n",
        "    **kwargs: Any\n",
        ") -> ResNet:\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
        "                   **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Training Function"
      ],
      "metadata": {
        "id": "48mKTQq3ilZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, device):\n",
        "\n",
        "    # Learning rate\n",
        "    learning_rate = 1e-2\n",
        "\n",
        "    # No. of epochs\n",
        "    num_epochs = 10\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Model Training\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate through the dataloader\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        # Model Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "\n",
        "        print(\"Epoch: {:02d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "QxBD7txEighQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Test Function"
      ],
      "metadata": {
        "id": "o0kT-zi4jEo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device, criterion=None):\n",
        "    # Setting model mode to eval()\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterating through the test dataloader\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels).item()\n",
        "        else:\n",
        "            loss = 0\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    eval_loss = running_loss / len(test_loader.dataset)\n",
        "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "    return eval_loss, eval_accuracy"
      ],
      "metadata": {
        "id": "p0tbCRMPGK1s"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-training quantization (PTQ) can reduce the memory footprint and latency of deep model inference, while still\n",
        "# preserving the accuracy of the model, with only a small unlabeled calibration set and without the retraining on full training set.\n",
        "# To calibrate a quantized model, current PTQ methods usually randomly select some unlabeled data from training set as calibration data.\n",
        "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        _ = model(inputs)\n",
        "\n",
        "# how to measure inference time correctly\n",
        "# measure the time for model inference\n",
        "def measure_inference_latency(model,\n",
        "                              device,\n",
        "                              input_size=(1, 3, 32, 32),\n",
        "                              num_samples=100,\n",
        "                              num_warmups=10):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    x = torch.rand(size=input_size).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_warmups):\n",
        "            _ = model(x)\n",
        "    # WAIT FOR GPU SYNC\n",
        "    # torch.cuda.synchronize(). This line of code performs synchronization between the host and device (i.e., GPU and CPU),\n",
        "    # so the time recording takes place only after the process running on the GPU is finished.\n",
        "    # Waits for everything to finish running\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    with torch.no_grad(): # No gradients calculation\n",
        "        # Start time\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_samples):\n",
        "            _ = model(x)\n",
        "            torch.cuda.synchronize()\n",
        "        # Inference time took for the model to run the samples on GPU\n",
        "        end_time = time.time()\n",
        "    # Difference between start and end time\n",
        "    elapsed_time = end_time - start_time\n",
        "    # Average elapsed time\n",
        "    elapsed_time_ave = elapsed_time / num_samples\n",
        "\n",
        "    return elapsed_time_ave"
      ],
      "metadata": {
        "id": "w7tkSSlGjWkJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save and load the models"
      ],
      "metadata": {
        "id": "ltA8B4_YqAJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the the model\n",
        "def save_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.save(model.state_dict(), model_filepath)\n",
        "\n",
        "# Load the model using pytorch state dict\n",
        "def load_model(model, model_filepath, device):\n",
        "\n",
        "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Save an offline version of this module for use in a separate process.\n",
        "# The saved module serializes all of the methods, submodules, parameters, and attributes of this module.\n",
        "# https://pytorch.org/docs/stable/generated/torch.jit.save.html\n",
        "def save_torchscript_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
        "\n",
        "# All previously saved modules, no matter their device, are first loaded onto CPU, and then are moved to the devices they were saved from\n",
        "# https://pytorch.org/docs/stable/generated/torch.jit.load.html\n",
        "def load_torchscript_model(model_filepath, device):\n",
        "\n",
        "    model = torch.jit.load(model_filepath, map_location=device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Ozz4IsLFjaxv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the model"
      ],
      "metadata": {
        "id": "N5u_C0Q6qtgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model(without pretraining) with no of clasess\n",
        "def create_model(num_classes=10):\n",
        "    model = resnet18(num_classes=num_classes, pretrained=False)\n",
        "    return model\n",
        "\n",
        "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
        "\n",
        "    model_1.to(device)\n",
        "    model_2.to(device)\n",
        "\n",
        "    for _ in range(num_tests):\n",
        "        x = torch.rand(size=input_size).to(device)\n",
        "        y1 = model_1(x).detach().cpu().numpy()\n",
        "        y2 = model_2(x).detach().cpu().numpy()\n",
        "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
        "            print(\"Model equivalence test sample failed: \")\n",
        "            print(y1)\n",
        "            print(y2)\n",
        "            return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "3ZFM9Vx1jgDq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization\n",
        "\n",
        "* Quantization refers to techniques for doing both computations and memory accesses with lower precision data, usually **int8** compared to floating point implementations.\n",
        "    \n",
        "* Quantization leverages 8bit integer (int8) instructions to reduce the model size and run the inference faster (reduced latency).\n",
        "    \n",
        "* This enables providing quick inference from a trained model and even fitting it into the resources available on a mobile device.\n",
        "\n",
        "* Quantization allows for siginificant performance gains!\n",
        "    * Up to 4x reduction in model size.\n",
        "    * Up to 2-4x reduction in memory bandwidth.\n",
        "    * Up to 2-4x faster inference due to savings in memory bandwidth and faster compute with int8 arithmetic (the exact speed up varies depending on the hardware, the runtime, and the model).\n",
        "    \n",
        "* Quantization doesn't come without additional cost, as it means introducing approximations and the resulting networks have slightly less accuracy.\n",
        "    \n",
        "* These techniques attempt to minimize the gap between the full floating point accuracy and the quantized accuracy.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/d94bdf2f6668d55de89d3c4b602b971e1fb1699c//assets/tut_compress_quant.png\" width=750px/>\n",
        "</center>\n",
        "\n",
        "\n",
        "* Quantization is available in PyTorch in various flavors starting in version 1.3 and there are published quantized models for ResNet, ResNext, MobileNetV2, GoogleNet, InceptionV3 and ShuffleNetV2 in the PyTorch torchvision 0.5 library.\n",
        "\n",
        "* Quantization is compatible with the rest of PyTorch: quantized models are traceable and scriptable. Quantized and floating point operations can be mixed in a model.\n",
        "\n",
        "* Mapping of floating point tensors to quantized tensors is customizable with user defined observer/fake-quantization blocks. PyTorch provides default implementations that should work for most use cases.\n",
        "\n",
        "* Currently the quantized models can only be run on CPU. However, it is possible to send the non-quantized parts of the model to a GPU.\n",
        "    * GPU quantization is a work-in-progress, see PTQ (Post Training Quantization)\n",
        "\n"
      ],
      "metadata": {
        "id": "QFZN0QvZzmOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define a quantized version of our network.\n",
        "\n",
        "For this, apply `torch.quantization.QuantStub()` and `torch.quantization.DequantStub()` to the inputs and outputs, respectively.\n",
        "\n",
        "**Note**:  This step is to ask PyTorch to specifically collect quantization statistics for the inputs and outputs, respectively. Otherwise, since PyTorch collects quantization statistics for weights and activations by default, there will be problems for the input quantization and output dequantization, since there are no quantization statistics collected for them."
      ],
      "metadata": {
        "id": "Fqruslkzb7tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a floating point model where some layers could be statically quantized\n",
        "class QuantizedResNet18(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QuantizedResNet18, self).__init__()\n",
        "\n",
        "        # QuantStub converts tensors from floating point to quantized.\n",
        "        # This will only be used for inputs.\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "\n",
        "        # DeQuantStub converts tensors from quantized to floating point.\n",
        "        # This will only be used for outputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "        # FP32 model\n",
        "        # FP32 is a FP32 Floating point data format for Deep Learning where data is represented as a 32-bit floating point number.\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # manually specify where tensors will be converted from floating\n",
        "        # point to quantized in the quantized model\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "\n",
        "        # manually specify where tensors will be converted from quantized\n",
        "        # to floating point in the quantized model\n",
        "        x = self.dequant(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "T172-SvRcCK8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post training static quantization\n",
        "\n",
        "Static quantization first feeds batches of data through the network and computes the resulting distributions of the different activations.\n",
        "Static quantization quantizes the weights and activations of the model. It fuses activations into preceding layers wherever possible. It requires calibration with a representative dataset to determine optimal quantization parameters for activations. Post Training Quantization is typically used when both memory bandwidth and compute savings are important with CNNs being a typical use case. Static quantization is also known as **Post Training Quantization** or **PTQ**.\n",
        "\n",
        "Steps involved in post training static quantization are :\n",
        "1.  Load pretrained model or train a model.\n",
        "2.  Fuse modules - combine operations/modules into a single module to obtain higher accuracy and performance. This is done using the fuse_modules() API, which takes in lists of modules to be fused. We currently support the following fusions: [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]\n",
        "3.  Evaluate model in calibration dataset.\n",
        "4.  Calibration data is used to calibrate the model. It is usually a subset of training data.\n",
        "5.  Calculate dynamic ranges of weights and activations in the network to calculate quantization parameters(scale and zero point).\n",
        "6.  Quantize the network using quantization parameters and run the inference.\n",
        "\n",
        "\n",
        "Apply quantization after training, quantization parameters are calculated based on sample calibration data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qXC8E1GccL6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 0\n",
        "num_classes = 10\n",
        "cuda_device = torch.device(\"cuda:0\")\n",
        "cpu_device = torch.device(\"cpu:0\")\n",
        "\n",
        "model_dir = \"saved_models\"\n",
        "model_filename = \"resnet18_cifar10.pt\"\n",
        "quantized_model_filename = \"resnet18_quantized_cifar10.pt\"\n",
        "model_filepath = os.path.join(model_dir, model_filename)\n",
        "quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
        "\n",
        "set_random_seeds(random_seed=random_seed)\n",
        "\n",
        "# Create an untrained model.\n",
        "model = create_model(num_classes=num_classes)\n",
        "\n",
        "train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
        "\n",
        "# Train model.\n",
        "model = train_model(model=model, train_loader=train_loader, test_loader=test_loader, device=cuda_device)\n",
        "\n",
        "# Save model.\n",
        "save_model(model=model, model_dir=model_dir, model_filename=model_filename)\n",
        "\n",
        "# Load a pretrained model.\n",
        "model = load_model(model=model, model_filepath=model_filepath, device=cuda_device)\n",
        "\n",
        "# Move the model to CPU since static quantization does not support CUDA currently.\n",
        "model.to(cpu_device)\n",
        "\n",
        "# Make a copy of the model for layer fusion\n",
        "fused_model = copy.deepcopy(model)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# The model has to be switched to evaluation mode before any layer fusion.\n",
        "# Otherwise the quantization will not work correctly.\n",
        "fused_model.eval()"
      ],
      "metadata": {
        "id": "VxW2caO-Fe_C",
        "outputId": "f9eb9937-dd15-4c56-af8f-c4ff8f2ef1c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 170498071/170498071 [00:01<00:00, 101325186.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "Epoch: 00 Train Loss: 1.707 Train Acc: 0.382 Eval Loss: 1.425 Eval Acc: 0.487\n",
            "Epoch: 01 Train Loss: 1.365 Train Acc: 0.505 Eval Loss: 1.166 Eval Acc: 0.581\n",
            "Epoch: 02 Train Loss: 1.176 Train Acc: 0.581 Eval Loss: 1.083 Eval Acc: 0.618\n",
            "Epoch: 03 Train Loss: 1.059 Train Acc: 0.628 Eval Loss: 0.945 Eval Acc: 0.670\n",
            "Epoch: 04 Train Loss: 0.968 Train Acc: 0.658 Eval Loss: 0.872 Eval Acc: 0.691\n",
            "Epoch: 05 Train Loss: 0.900 Train Acc: 0.683 Eval Loss: 0.836 Eval Acc: 0.713\n",
            "Epoch: 06 Train Loss: 0.852 Train Acc: 0.700 Eval Loss: 0.914 Eval Acc: 0.705\n",
            "Epoch: 07 Train Loss: 0.814 Train Acc: 0.711 Eval Loss: 0.850 Eval Acc: 0.715\n",
            "Epoch: 08 Train Loss: 0.773 Train Acc: 0.728 Eval Loss: 0.800 Eval Acc: 0.717\n",
            "Epoch: 09 Train Loss: 0.741 Train Acc: 0.739 Eval Loss: 0.737 Eval Acc: 0.742\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusing batchnorm with convolution saves extra computation at time of inference. It is also known as `Batchnorm folding`**.\n",
        "\n",
        "We update the weight and bias  of convolution layer before batchnorm to simulate the effect of batchnorm.\n",
        "\n",
        "Equations (5) & (6) denote the same  \n",
        "\n",
        "<br>\n",
        "\n",
        "![bn_folding.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "ygkiYBoQ8sVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fuse the model in place rather manually.\n",
        "# Fuse the activations to preceding layers, where applicable.\n",
        "# This needs to be done manually depending on the model architecture.\n",
        "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
        "# https://pytorch.org/docs/stable/generated/torch.quantization.fuse_modules.html\n",
        "# It returns model with fused modules. A new copy is created if inplace=True.\n",
        "fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
        "for module_name, module in fused_model.named_children():\n",
        "    if \"layer\" in module_name:\n",
        "        for basic_block_name, basic_block in module.named_children():\n",
        "            torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
        "            for sub_block_name, sub_block in basic_block.named_children():\n",
        "                if sub_block_name == \"downsample\":\n",
        "                    torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
        "\n",
        "# Print FP32 model.\n",
        "print(model)\n",
        "\n",
        "# Print fused model.\n",
        "print(fused_model)\n",
        "\n",
        "# Model and fused model should be equivalent.\n",
        "assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\""
      ],
      "metadata": {
        "id": "EgzFQyjY-Hwg",
        "outputId": "e1a5f47d-e96e-4cd5-9c10-9d461490b8bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "ResNet(\n",
            "  (conv1): ConvReLU2d(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (bn1): Identity()\n",
            "  (relu): Identity()\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Converting networks to use both integer arithmetic and int8 memory accesses can improve the latency performance.\n",
        "    \n",
        "* Static quantization first feeds batches of data through the network and computes the resulting distributions of the different activations.\n",
        "    * This is done by inserting observer modules at different points that record these distributions.\n",
        "    \n",
        "* This information is used to determine how specifically the different activations should be quantized at inference time.\n",
        "        \n",
        "* A simple technique would be to simply divide the entire range of activations into 256 levels, but PyTorch supports more sophisticated methods as well.\n",
        "    \n",
        "* This step allows to pass quantized values between operations instead of converting these values to floats - and then back to ints - between every operation, resulting in a significant speed-up.\n",
        "    \n",
        "* **Optimizing static quantization includes:**\n",
        "    * **Observers:** observer modules specify how statistics are collected prior to quantization to try out more advanced methods to quantize the data.\n",
        "\n",
        "    * **Operator fusion:** fuse multiple operations into a single operation, saving on memory access while also improving the operations numerical accuracy.\n",
        "    \n",
        "    * **Per-channel quantization:** we can independently quantize weights for each output channel in a convolution/linear layer, which can lead to higher accuracy with almost the same speed.\n"
      ],
      "metadata": {
        "id": "53Hwkm3J2WDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will perform the following:\n",
        "\n",
        "*  Specify quantization configurations, such as symmetric quantization or asymmetric quantization, etc.\n",
        "*  Prepare quantization model for post-training calibration.\n",
        "*  Run post-training calibration.\n",
        "*  Convert the calibrated floating point model to quantized integer model.\n",
        "*  Verify accuracies and inference performance gain.\n",
        "*  Save the quantized integer model."
      ],
      "metadata": {
        "id": "QstO3rQocSXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for static quantization. This inserts observers in\n",
        "# the model that will observe activation tensors during calibration.\n",
        "quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
        "\n",
        "# Using un-fused model will fail.\n",
        "# Because there is no quantized layer implementation for a single batch normalization layer.\n",
        "# quantized_model = QuantizedResNet18(model_fp32=model) # This will not work\n",
        "# Select quantization schemes from\n",
        "# https://pytorch.org/docs/stable/quantization-support.html\n",
        "# When preparing a quantized model, it is necessary to ensure that qconfig and the engine used for\n",
        "# quantized computations match the backend on which the model will be executed. The qconfig controls the type of\n",
        "# observers used during the quantization passes. The qengine controls whether fbgemm or qnnpack specific packing\n",
        "# function is used when packing weights for linear and convolution functions and modules.\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "\n",
        "# Custom quantization configurations\n",
        "# quantization_config = torch.quantization.default_qconfig\n",
        "# quantization_config = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.quint8), weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
        "\n",
        "quantized_model.qconfig = quantization_config\n",
        "\n",
        "# Print quantization configurations\n",
        "print(quantized_model.qconfig)\n",
        "\n",
        "torch.quantization.prepare(quantized_model, inplace=True)\n",
        "\n",
        "# Use training data for calibration.\n",
        "calibrate_model(model=quantized_model, loader=train_loader, device=cpu_device)\n",
        "\n",
        "quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
        "\n",
        "# Using high-level static quantization wrapper\n",
        "# The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
        "# quantized_model = torch.quantization.quantize(model=quantized_model, run_fn=calibrate_model, run_args=[train_loader], mapping=None, inplace=False)\n",
        "\n",
        "quantized_model.eval()\n",
        "\n",
        "# Print the quantized model.\n",
        "print(quantized_model)\n",
        "\n",
        "# Save the quantized model.\n",
        "save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)\n",
        "\n",
        "# Load the quantized model.\n",
        "quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
        "\n",
        "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "\n",
        "# Skip this assertion since the values might deviate a lot.\n",
        "# assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
        "\n",
        "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
        "\n",
        "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
        "\n",
        "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
      ],
      "metadata": {
        "id": "_LrHNHHuHXZ2",
        "outputId": "5afce1a8-8321-4095-f752-f59b7dae29d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
            "QuantizedResNet18(\n",
            "  (quant): Quantize(scale=tensor([0.0408]), zero_point=tensor([60]), dtype=torch.quint8)\n",
            "  (dequant): DeQuantize()\n",
            "  (model_fp32): ResNet(\n",
            "    (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.06955798715353012, zero_point=0, padding=(3, 3))\n",
            "    (bn1): Identity()\n",
            "    (relu): Identity()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.039114080369472504, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.08863971382379532, zero_point=61, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.13954873383045197, zero_point=35\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.038439881056547165, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.08596448600292206, zero_point=61, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.15826715528964996, zero_point=35\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.036637019366025925, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.0813264325261116, zero_point=62, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.08833520114421844, zero_point=64)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.12936443090438843, zero_point=63\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.042018961161375046, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.08775319159030914, zero_point=60, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.12707111239433289, zero_point=43\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.051526252180337906, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.10002413392066956, zero_point=52, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.09109745174646378, zero_point=62)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.14034844934940338, zero_point=57\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.07539132982492447, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.18783578276634216, zero_point=48, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.21367166936397552, zero_point=41\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.08641667664051056, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.2014591246843338, zero_point=62, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.15525218844413757, zero_point=63)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.3001069724559784, zero_point=63\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.11967351287603378, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.25179222226142883, zero_point=68, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.24048124253749847, zero_point=63\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.2691155970096588, zero_point=38, qscheme=torch.per_channel_affine)\n",
            "  )\n",
            ")\n",
            "FP32 evaluation accuracy: 0.742\n",
            "INT8 evaluation accuracy: 0.739\n",
            "FP32 CPU Inference Latency: 11.51 ms / sample\n",
            "FP32 CUDA Inference Latency: 3.35 ms / sample\n",
            "INT8 CPU Inference Latency: 5.72 ms / sample\n",
            "INT8 JIT CPU Inference Latency: 1.57 ms / sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization aware training\n",
        "Static quantization allows the user to generate quantized integer model that is highly efficient during inference. However, sometimes, even with careful post-training calibration, the model accuracies might be sacrificed to some extent that is not acceptable. If this is the case, post-training calibration is not sufficient to generate a quantized integer model. We would have train the model in a way so that the quantization effect has been taken into account. Quantization aware training is capable of modeling the quantization effect during training.\n",
        "\n",
        "The mechanism of quantization aware training is simple, it places fake quantization modules, i.e., all weights and activations are fake quantized during both the forward and backward passes of training: quantization and dequantization modules, at the places where quantization happens during floating-point model to quantized integer model conversion, to simulate the effects of clamping and rounding brought by integer quantization. The fake quantization modules will also monitor scales and zero points of the weights and activations. Once the quantization aware training is finished, the floating point model could be converted to quantized integer model immediately using the information stored in the fake quantization modules."
      ],
      "metadata": {
        "id": "B9LMCuRRNIZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_qat(model, train_loader, test_loader, device, learning_rate=1e-1, num_epochs=5):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1, last_epoch=-1)\n",
        "\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "    # Evaluation\n",
        "    # model.eval()\n",
        "    # eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "    # print(\"Epoch: {:02d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(-1, eval_loss, eval_accuracy))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "\n",
        "        if epoch > 3:\n",
        "        # Freeze quantizer parameters\n",
        "          model.apply(torch.quantization.disable_observer)\n",
        "\n",
        "        if epoch > 2:\n",
        "        # Freeze batch norm mean and variance estimates\n",
        "          model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "\n",
        "        # Set learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "OpOUfWFT7npm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model_filename = \"resnet18_QAT_quantized_cifar10.pt\"\n",
        "quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
        "\n",
        "set_random_seeds(random_seed=random_seed)\n",
        "\n",
        "# Create an untrained model.\n",
        "model = create_model(num_classes=num_classes)\n",
        "\n",
        "train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
        "\n",
        "# # Load a pretrained model.\n",
        "model = load_model(model=model, model_filepath=model_filepath, device=cuda_device)\n",
        "\n",
        "# Move the model to CPU since static quantization does not support CUDA currently.\n",
        "model.to(cpu_device)\n",
        "\n",
        "# Make a copy of the model for layer fusion\n",
        "fused_model = copy.deepcopy(model)\n",
        "\n",
        "# Model and fused model should be equivalent.\n",
        "model.eval()\n",
        "fused_model.eval()\n",
        "\n",
        "\n",
        "# Fuse the model in place rather manually.\n",
        "fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
        "for module_name, module in fused_model.named_children():\n",
        "    if \"layer\" in module_name:\n",
        "        for basic_block_name, basic_block in module.named_children():\n",
        "            torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
        "            for sub_block_name, sub_block in basic_block.named_children():\n",
        "                if sub_block_name == \"downsample\":\n",
        "                    torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
        "\n",
        "# Print FP32 model.\n",
        "print(model)\n",
        "\n",
        "# Print fused model.\n",
        "print(fused_model)\n",
        "\n",
        "assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\""
      ],
      "metadata": {
        "id": "0U8gTtVnHYlH",
        "outputId": "bdb9f2bd-ef82-4220-d34a-ba087938883b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "ResNet(\n",
            "  (conv1): ConvReLU2d(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (bn1): Identity()\n",
            "  (relu): Identity()\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the model for quantization aware training."
      ],
      "metadata": {
        "id": "Gsd4vE8m3IT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for quantization aware training. This inserts observers in\n",
        "# the model that will observe activation tensors during calibration.\n",
        "quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
        "\n",
        "# Using un-fused model will fail.\n",
        "# Because there is no quantized layer implementation for a single batch normalization layer.\n",
        "# quantized_model = QuantizedResNet18(model_fp32=model)\n",
        "# Select quantization schemes from\n",
        "# https://pytorch.org/docs/stable/quantization-support.html\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "\n",
        "# Custom quantization configurations\n",
        "# quantization_config = torch.quantization.default_qconfig\n",
        "# quantization_config = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.quint8), weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
        "\n",
        "quantized_model.qconfig = quantization_config\n",
        "\n",
        "# Print quantization configurations\n",
        "print(quantized_model.qconfig)\n",
        "\n",
        "# https://pytorch.org/docs/stable/_modules/torch/quantization/quantize.html#prepare_qat\n",
        "torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
        "\n",
        "# # Use training data for calibration.\n",
        "print(\"Training QAT Model...\")\n",
        "quantized_model.train()\n",
        "\n",
        "train_model_qat(model=quantized_model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-3, num_epochs=5)\n",
        "quantized_model.to(cpu_device)\n",
        "\n",
        "# Using high-level static quantization wrapper\n",
        "# The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
        "# quantized_model = torch.quantization.quantize_qat(model=quantized_model, run_fn=train_model, run_args=[train_loader, test_loader, cuda_device], mapping=None, inplace=False)\n",
        "\n",
        "# Convert the observed model to a quantized model. This does several things:\n",
        "# quantizes the weights, computes and stores the scale and bias value to be\n",
        "# used with each activation tensor, fuses modules where appropriate,\n",
        "# and replaces key operators with quantized implementations.\n",
        "quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
        "quantized_model.eval()\n",
        "\n",
        "# Print quantized model.\n",
        "print(quantized_model)\n",
        "\n",
        "# Save quantized model.\n",
        "save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)\n",
        "\n",
        "# Load quantized model.\n",
        "quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
        "\n",
        "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "\n",
        "# Skip this assertion since the values might deviate a lot.\n",
        "# assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
        "\n",
        "print(\"QAT FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "print(\"QAT INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
        "\n",
        "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
        "\n",
        "print(\"QAT FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "print(\"QAT FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "print(\"QAT INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "print(\"QAT INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
      ],
      "metadata": {
        "id": "7HwZae8GJ8KY",
        "outputId": "29b89c98-3889-4a93-f5d3-9bfe645389a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
            "Training QAT Model...\n",
            "Epoch: 000 Train Loss: 0.658 Train Acc: 0.768 Eval Loss: 0.689 Eval Acc: 0.761\n",
            "Epoch: 001 Train Loss: 0.636 Train Acc: 0.775 Eval Loss: 0.679 Eval Acc: 0.763\n",
            "Epoch: 002 Train Loss: 0.615 Train Acc: 0.781 Eval Loss: 0.674 Eval Acc: 0.763\n",
            "Epoch: 003 Train Loss: 0.605 Train Acc: 0.785 Eval Loss: 0.659 Eval Acc: 0.770\n",
            "Epoch: 004 Train Loss: 0.602 Train Acc: 0.786 Eval Loss: 0.665 Eval Acc: 0.771\n",
            "QuantizedResNet18(\n",
            "  (quant): Quantize(scale=tensor([0.0408]), zero_point=tensor([60]), dtype=torch.quint8)\n",
            "  (dequant): DeQuantize()\n",
            "  (model_fp32): ResNet(\n",
            "    (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.0701265037059784, zero_point=0, padding=(3, 3))\n",
            "    (bn1): Identity()\n",
            "    (relu): Identity()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.04138147085905075, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.08008143305778503, zero_point=64, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.15034383535385132, zero_point=33\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.04038740694522858, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.08833694458007812, zero_point=63, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.13936449587345123, zero_point=39\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.04813766852021217, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.08295907825231552, zero_point=63, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.0955023244023323, zero_point=62)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.1314743459224701, zero_point=62\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.04827504605054855, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.089625284075737, zero_point=60, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.1302538365125656, zero_point=43\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.051860082894563675, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.09724976122379303, zero_point=53, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.097618468105793, zero_point=62)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.13697832822799683, zero_point=56\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.06287357956171036, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.16510966420173645, zero_point=51, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.20154552161693573, zero_point=41\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.08183276653289795, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.1800520271062851, zero_point=63, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.14654651284217834, zero_point=62)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.27128228545188904, zero_point=64\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.1035168319940567, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.21283815801143646, zero_point=67, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.2176348865032196, zero_point=63\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.22354862093925476, zero_point=44, qscheme=torch.per_channel_affine)\n",
            "  )\n",
            ")\n",
            "QAT FP32 evaluation accuracy: 0.742\n",
            "QAT INT8 evaluation accuracy: 0.765\n",
            "QAT FP32 CPU Inference Latency: 7.96 ms / sample\n",
            "QAT FP32 CUDA Inference Latency: 3.50 ms / sample\n",
            "QAT INT8 CPU Inference Latency: 2.81 ms / sample\n",
            "QAT INT8 JIT CPU Inference Latency: 1.46 ms / sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pruning using PyTorch"
      ],
      "metadata": {
        "id": "XvWpgt4JjV4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning models these days require a significant amount of computing, memory, and power which becomes a bottleneck in the conditions where we need real-time inference or to run models on edge devices and browsers with limited computational resources. Energy efficiency is a major concern for current deep learning models. One of the methods for tackling this efficiency is enabling inference efficiency.\n",
        "\n",
        "**Larger Model => More Memory References => More Energy**\n",
        "\n",
        "Pruning is one of the methods for inference to efficiently produce models smaller in size, more memory-efficient, more power-efficient and faster at inference with minimal loss in accuracy, other such techniques being weight sharing and quantization. Out of several aspects that deep learning takes as an inspiration from the area of Neuroscience. Moreover, pruning in deep learning is also a biologically inspired concept. Now, let us begin with it's implementation.\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/720/1*nicFUkeUpWMW1w_hUVtZiw.png\" width=650px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "L0Hg1PF-jVRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required libraries"
      ],
      "metadata": {
        "id": "teKufNMwi3Fw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNk2Xgu6UTG3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "import copy\n",
        "import pandas as pd"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX_iW6N-sLkF"
      },
      "source": [
        "### Effects of Pruning Neural Nets\n",
        "\n",
        "\n",
        "\n",
        "Pruning is deleting connections in a neural net in order to improve generalization and reduce computational resources.\n",
        "Two most common type of pruning are :\n",
        "\n",
        "1.   Weight-pruning\n",
        "2.   Unit-pruning\n",
        "\n",
        "In **weight pruning**  the largest weights by absolute value are set to zero.\n",
        "While in **unit-pruning** , the smallest neurons are set to zero by a vector-wise metric like **L2-norm**.\n",
        "\n",
        "Here, we examine the relationship between pruning and accuracy on a vanilla neural net. Before running any experiments, we hypothesize that accuracy for the pruned neural net will slightly rise (due to the regularization), and then have a negative linear correlation with the amount pruned. we also hypothesize that unit-pruning, in deleting entire neurons instead of individual weights, will have a more dramatic negative effect than weight-pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZmejOzw8MHS"
      },
      "source": [
        "First, let's load, normalize, and visualize the MNIST dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5QgQR6UUlwF"
      },
      "source": [
        "def load_MNIST():\n",
        "  \"\"\"Function to load and normalize MNIST data\"\"\"\n",
        "  train = torchvision.datasets.MNIST(root='./data', download=True, train=True, transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)),\n",
        "                                ]))\n",
        "  test = torchvision.datasets.MNIST(root='./data', download=True, train=False, transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)),\n",
        "                                ]))\n",
        "  print(\"MNIST datset loaded and normalized.\")\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=train, shuffle=True, batch_size=100)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset=test, shuffle=False, batch_size=100)\n",
        "  print(\"PyTorch DataLoaders loaded.\")\n",
        "  return train, test, train_loader, test_loader"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X3rzpxCXv4R"
      },
      "source": [
        "def visualize_MNIST(train_loader):\n",
        "  \"\"\"Function to visualize data given a DataLoader object\"\"\"\n",
        "  dataiter = iter(train_loader)\n",
        "  images, labels = next(dataiter)\n",
        "  print(\"image shape:\", images.shape, \"\\n label shape:\", labels.shape)\n",
        "  # visualize data\n",
        "  fig, ax = plt.subplots(2,5)\n",
        "  for i, ax in enumerate(ax.flatten()):\n",
        "      im_idx = np.argwhere(labels == i)[0][0]\n",
        "      plottable_image = images[im_idx].squeeze()\n",
        "      ax.imshow(plottable_image)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awglncFta6ga",
        "outputId": "57b48400-a2b2-4657-9a78-9f182abbf51f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        }
      },
      "source": [
        "# load and visualize MNIST\n",
        "train, test, train_loader, test_loader = load_MNIST()\n",
        "visualize_MNIST(train_loader)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9912422/9912422 [00:00<00:00, 118241470.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 28881/28881 [00:00<00:00, 88809159.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1648877/1648877 [00:00<00:00, 32426651.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 4542/4542 [00:00<00:00, 21190799.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "MNIST datset loaded and normalized.\n",
            "PyTorch DataLoaders loaded.\n",
            "image shape: torch.Size([100, 1, 28, 28]) \n",
            " label shape: torch.Size([100])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avoio0W48Yzi"
      },
      "source": [
        "### Build Vanilla Neural Network\n",
        "\n",
        "Now let's build a vanilla neural net with four hidden layers without pruning.\n",
        "\n",
        "We'll keep things simple and leave out biases, convolutions, and pooling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnUxUg7iU4yn"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  \"\"\"A non-sparse neural network with four hidden fully-connected layers\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    self.input_layer = nn.Linear(784, 1000, bias=False)\n",
        "    self.hidden1_layer = nn.Linear(1000, 1000, bias=False)\n",
        "    self.hidden2_layer = nn.Linear(1000, 500, bias=False)\n",
        "    self.hidden3_layer = nn.Linear(500, 200, bias=False)\n",
        "    self.hidden4_layer = nn.Linear(200, 10, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.input_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden1_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden2_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden3_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden4_layer(x)\n",
        "    output = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqA-fHwd8toX"
      },
      "source": [
        "### Model Training\n",
        "\n",
        "Let's train our vanilla neural net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJcwD5-NeJBS"
      },
      "source": [
        "def train(model, train_loader, epochs=3, learning_rate=0.001):\n",
        "  \"\"\"Function to train a neural net\"\"\"\n",
        "\n",
        "  lossFunction = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  time0 = time()\n",
        "  total_samples = 0\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(\"Starting epoch\", e)\n",
        "    total_loss = 0\n",
        "\n",
        "    for idx, (images,labels) in enumerate(train_loader):\n",
        "      images = images.view(images.shape[0],-1) # flatten\n",
        "      optimizer.zero_grad() # zero out the gradients\n",
        "      output = model(images) # forward pass\n",
        "      loss = lossFunction(output,labels) # calculate loss\n",
        "      loss.backward() # backpropagate\n",
        "      optimizer.step() # update weights\n",
        "\n",
        "      total_samples += labels.size(0)\n",
        "      # Compute the loss\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if idx % 100 == 0:\n",
        "        print(\"Running loss:\", total_loss)\n",
        "\n",
        "  final_time = (time()-time0)/60\n",
        "  print(\"Model trained in \", final_time, \"minutes on \", total_samples, \"samples\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51kd-weohlPf",
        "outputId": "cb7d6879-7abf-46d3-ca93-52f43f361af7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = Net()\n",
        "train(model, train_loader)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 0\n",
            "Running loss: 2.301912784576416\n",
            "Running loss: 74.10412240028381\n",
            "Running loss: 106.3925986289978\n",
            "Running loss: 132.81659303605556\n",
            "Running loss: 155.05538753420115\n",
            "Running loss: 176.53194750100374\n",
            "Starting epoch 1\n",
            "Running loss: 0.1646682322025299\n",
            "Running loss: 16.16992173716426\n",
            "Running loss: 31.860308431088924\n",
            "Running loss: 47.339561343193054\n",
            "Running loss: 62.5030517950654\n",
            "Running loss: 75.86018631234765\n",
            "Starting epoch 2\n",
            "Running loss: 0.2028014063835144\n",
            "Running loss: 12.988306557759643\n",
            "Running loss: 24.516527350991964\n",
            "Running loss: 35.20362804085016\n",
            "Running loss: 46.668342024087906\n",
            "Running loss: 57.74598481506109\n",
            "Model trained in  2.1640615304311117 minutes on  180000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArL0CsYL83AB"
      },
      "source": [
        "### Model Testing\n",
        "\n",
        "Now we'll test our vanilla neural net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u61CAZip1UNx"
      },
      "source": [
        "def test(model, test_loader):\n",
        "  \"\"\"Test neural net\"\"\"\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, (images, labels) in enumerate(test_loader):\n",
        "      images = images.view(images.shape[0],-1) # flatten\n",
        "      output = model(images) # Forward pass\n",
        "      values, indices = torch.max(output.data, 1) # maximum probability of the predictions\n",
        "      total += labels.size(0)\n",
        "      correct += (labels == indices).sum().item()\n",
        "\n",
        "    # Accuracy of model\n",
        "    acc = correct / total * 100\n",
        "    # print(\"Accuracy: \", acc, \"% for \", total, \"training samples\")\n",
        "\n",
        "  return acc"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uAz77644j08",
        "outputId": "5e390f16-c88c-4942-aac1-7aa0f6b5531b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "acc = test(model, test_loader)\n",
        "print(\"The accuracy of our vanilla NN is\", acc, \"%\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of our vanilla NN is 96.63000000000001 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8QEw_mWGA3D"
      },
      "source": [
        "A ~96% accuracy for our vanilla neural network seems reasonable. Now let's do some weight and unit pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function for weight pruning"
      ],
      "metadata": {
        "id": "mao97ZAki8H2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3EavCDY4mOg"
      },
      "source": [
        "def sparsify_by_weights(model, k):\n",
        "\n",
        "  \"\"\"Function that takes un-sparsified neural net and does weight-pruning\n",
        "  by k sparsity\"\"\"\n",
        "\n",
        "  # make copy of original neural net\n",
        "  sparse_m = copy.deepcopy(model)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, i in enumerate(sparse_m.parameters()):\n",
        "      if idx == 4: # skip last layer of 5-layer neural net\n",
        "        break\n",
        "      # change tensor to numpy format, then set appropriate number of smallest weights to zero\n",
        "      layer_copy = torch.flatten(i)\n",
        "      layer_copy = layer_copy.detach().numpy()\n",
        "      indices = abs(layer_copy).argsort() # get indices of smallest weights by absolute value\n",
        "      indices = indices[:int(len(indices)*k)] # get k fraction of smallest indices\n",
        "      layer_copy[indices] = 0\n",
        "\n",
        "      # change weights of model\n",
        "      i = torch.from_numpy(layer_copy)\n",
        "\n",
        "  return sparse_m"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function for unit pruning"
      ],
      "metadata": {
        "id": "B1-8zJjojErP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTb5ptg80hqV"
      },
      "source": [
        "def l2(array):\n",
        "  return np.sqrt(np.sum([i**2 for i in array]))\n",
        "\n",
        "def sparsify_by_unit(model, k):\n",
        "  \"\"\"Creates a k-sparsity model with unit-level pruning that sets columns with smallest L2 to zero.\"\"\"\n",
        "\n",
        "  # make copy of original neural net\n",
        "  sparse_m = copy.deepcopy(model)\n",
        "\n",
        "  for idx, i in enumerate(sparse_m.parameters()):\n",
        "    if idx == 4: # skip last layer of 5-layer neural net\n",
        "      break\n",
        "    layer_copy = i.detach().numpy()\n",
        "    indices = np.argsort([l2(i) for i in layer_copy])\n",
        "    indices = indices[:int(len(indices)*k)]\n",
        "    layer_copy[indices,:] = 0\n",
        "    i = torch.from_numpy(layer_copy)\n",
        "\n",
        "  return sparse_m"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX21TSoq3udv"
      },
      "source": [
        "def get_pruning_accuracies(model, prune_type):\n",
        "  \"\"\" Takes a model and prune type (\"weight\" or \"unit\") and returns a DataFrame of pruning accuracies for given sparsities.\"\"\"\n",
        "\n",
        "  df = pd.DataFrame({\"sparsity\": [], \"accuracy\": []})\n",
        "  sparsities = [0.0, 0.25, 0.50, 0.60, 0.70, 0.80]#, 0.90, 0.95, 0.97, 0.99]\n",
        "\n",
        "  for s in sparsities:\n",
        "    if prune_type == \"weight\":\n",
        "      new_model = sparsify_by_weights(model, s)\n",
        "    elif prune_type == \"unit\":\n",
        "      new_model = sparsify_by_unit(model, s)\n",
        "    else:\n",
        "      print(\"Must specify prune-type.\")\n",
        "      return\n",
        "    acc = test(new_model, test_loader)\n",
        "    df = df.append({\"sparsity\": s, \"accuracy\": acc}, ignore_index=True)\n",
        "\n",
        "  return df,new_model"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIWGnpQZHYSi"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WgshrNX5Is2",
        "outputId": "084e21d2-db72-4503-ef0e-d6b414ff1242",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_weight,weight_pruned_model = get_pruning_accuracies(model, \"weight\")\n",
        "df_unit,unit_pruned_model = get_pruning_accuracies(model, \"unit\")\n",
        "\n",
        "print(\"Accuracies for Weight Pruning\")\n",
        "print(df_weight)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Accuracies for Unit Pruning\")\n",
        "print(df_unit)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies for Weight Pruning\n",
            "   sparsity  accuracy\n",
            "0      0.00     96.63\n",
            "1      0.25     96.73\n",
            "2      0.50     96.64\n",
            "3      0.60     96.58\n",
            "4      0.70     96.76\n",
            "5      0.80     95.07\n",
            "\n",
            "Accuracies for Unit Pruning\n",
            "   sparsity  accuracy\n",
            "0      0.00     96.63\n",
            "1      0.25     96.61\n",
            "2      0.50     96.46\n",
            "3      0.60     95.92\n",
            "4      0.70     93.21\n",
            "5      0.80     74.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's visualize weights and compare it with unpruned model."
      ],
      "metadata": {
        "id": "XHCjIOdNOdio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_weights(weights, n_weights):\n",
        "\n",
        "    rows = int(np.sqrt(n_weights))\n",
        "    cols = int(np.sqrt(n_weights))\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    for i in range(rows*cols):\n",
        "        ax = fig.add_subplot(rows, cols, i+1)\n",
        "        ax.imshow(weights[i].view(28, 28).cpu().numpy(), cmap='bone')\n",
        "        ax.axis('off')\n",
        "N_WEIGHTS = 9\n",
        "\n",
        "weights = model.input_layer.weight.data\n",
        "\n",
        "plot_weights(weights, N_WEIGHTS)"
      ],
      "metadata": {
        "id": "N9I-ciGKLXMu",
        "outputId": "bb88422e-7fa0-40d7-9768-b214f39000a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets visualize weights after pruning\n",
        "# Weight pruning at 80 percent sparsity\n",
        "N_WEIGHTS = 9\n",
        "\n",
        "weights = weight_pruned_model.input_layer.weight.data\n",
        "\n",
        "plot_weights(weights, N_WEIGHTS)"
      ],
      "metadata": {
        "id": "bNpmr_LVNY_1",
        "outputId": "8b9447f4-bf66-40c0-a2f3-01537183c0fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets visualize weights after pruning\n",
        "\n",
        "# Unit pruning at 80 percent sparsity\n",
        "N_WEIGHTS = 9\n",
        "\n",
        "weights = unit_pruned_model.input_layer.weight.data\n",
        "\n",
        "plot_weights(weights, N_WEIGHTS)"
      ],
      "metadata": {
        "id": "JTTzx1kQN-Zc",
        "outputId": "7f8f084f-4918-4a04-bff9-b0f784134034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVBWjZMMDglS",
        "outputId": "390fbd97-2aff-4578-9ad2-0940af2eb4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "plt.figure()\n",
        "plt.title(\"Accuracy vs Sparsity\")\n",
        "plt.plot(df_unit[\"sparsity\"], df_unit[\"accuracy\"], label=\"Unit-pruning\")\n",
        "plt.plot(df_weight[\"sparsity\"], df_weight[\"accuracy\"], label=\"Weight-pruning\")\n",
        "plt.xlabel(\"Sparsity (as fraction)\")\n",
        "plt.ylabel(\"% Accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7b654f7dfe80>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterative Pruning"
      ],
      "metadata": {
        "id": "v0o38Rn74CfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike One-time  pruning + fine-tuning which achieves the desired prune rate by pruning and fine-tuning once, multi-time iterative pruning + fine-tuning achieves the desired prune rate by pruning and fine-tuning multiple-times.\n",
        "\n",
        "For example, to achieve the desired prune rate of\n",
        "98\n",
        "%\n",
        ", we could run pruning and fine-tuning for many iterations, achieving prune rate of\n",
        "30\n",
        "%\n",
        ",\n",
        "50\n",
        "%\n",
        ",\n",
        "66\n",
        "%\n",
        ",\n",
        "76\n",
        "%\n",
        ",\n",
        "\n",
        ",\n",
        "98\n",
        "%\n",
        " in each iteration.\n",
        "\n",
        " So basically we **train for some epochs, prune slightly and train again**. We repeat this process until desired sparsity is reached. In this case we use masks (either 0 or 1 ) per layer to get remember the indices of pruned elements."
      ],
      "metadata": {
        "id": "EeLX9LKIJ9wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedLinear(nn.Linear):\n",
        "\t\"\"\" same as Linear except has a configurable mask on the weights \"\"\"\n",
        "\n",
        "\tdef __init__(self, in_features, out_features, bias=True):\n",
        "\t\tsuper().__init__(in_features, out_features, bias)\n",
        "\t\tself.register_buffer('mask', torch.ones(out_features, in_features))\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\treturn F.linear(input, self.mask * self.weight, self.bias)\n",
        "\n",
        "class MaskNet(nn.Module):\n",
        "  \"\"\"A non-sparse neural network with four hidden fully-connected layers\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MaskNet,self).__init__()\n",
        "    self.input_layer = MaskedLinear(784, 1000, bias=False)\n",
        "    self.hidden1_layer = MaskedLinear(1000, 1000, bias=False)\n",
        "    self.hidden2_layer = MaskedLinear(1000, 500, bias=False)\n",
        "    self.hidden3_layer = MaskedLinear(500, 200, bias=False)\n",
        "    self.hidden4_layer = MaskedLinear(200, 10, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.input_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden1_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden2_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden3_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden4_layer(x)\n",
        "    output = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "NDNO5rNOO_0r"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_model = MaskNet()\n",
        "train(masked_model, train_loader)"
      ],
      "metadata": {
        "id": "ssVHc3r3Rd6O",
        "outputId": "da8841ad-8083-4bad-b261-8ff07088dfda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 0\n",
            "Running loss: 2.3034982681274414\n",
            "Running loss: 76.18879228830338\n",
            "Running loss: 110.2517869323492\n",
            "Running loss: 135.149330586195\n",
            "Running loss: 156.996777869761\n",
            "Running loss: 179.48454108834267\n",
            "Starting epoch 1\n",
            "Running loss: 0.26755964756011963\n",
            "Running loss: 16.575115531682968\n",
            "Running loss: 32.03917044773698\n",
            "Running loss: 47.042292814701796\n",
            "Running loss: 61.75750296190381\n",
            "Running loss: 76.32663689181209\n",
            "Starting epoch 2\n",
            "Running loss: 0.04951794072985649\n",
            "Running loss: 12.452651092782617\n",
            "Running loss: 24.6075307559222\n",
            "Running loss: 36.777904810383916\n",
            "Running loss: 48.71636760979891\n",
            "Running loss: 58.79316691868007\n",
            "Model trained in  2.3814589937527972 minutes on  180000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = test(masked_model, test_loader)\n",
        "print(\"The accuracy of our masked vanilla NN is\", acc, \"%\")"
      ],
      "metadata": {
        "id": "CFyraaMvSHIQ",
        "outputId": "c7701090-5392-4d63-a00f-2c6358fd8fe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of our masked vanilla NN is 96.28 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_prune_rate(max_prune_rate=0.5,start_epoch=0,num_prune_epochs=8,total_epoch=30):\n",
        "    \"\"\"Function to calculate pruning ratio for different pruning epochs\"\"\"\n",
        "    final_prune_epoch = int(0.8*total_epoch) # change ratio to change final prune epoch\n",
        "    #num_prune_epochs = 8\n",
        "    prune_rates = [max_prune_rate*(1 - (1 - (i / num_prune_epochs))**3)\n",
        "                    for i in range(num_prune_epochs)]\n",
        "    prune_rates[-1] = max_prune_rate\n",
        "    prune_epochs = np.linspace(start_epoch, final_prune_epoch, num_prune_epochs).astype('i').tolist()\n",
        "\n",
        "    return prune_rates,prune_epochs\n",
        "\n",
        "def prune_iterative(model, prune_percentage):\n",
        "  \"\"\"Function that takes un-sparsified neural net and does weight-pruning\n",
        "  by k sparsity\"\"\"\n",
        "\n",
        "  # make copy of original neural net\n",
        "  sparse_m = copy.deepcopy(model)\n",
        "  linear_layers = [m for m in sparse_m.modules() if isinstance(m,MaskedLinear) ]\n",
        "  with torch.no_grad():\n",
        "    for idx,layer in enumerate(linear_layers):\n",
        "      if idx == 4: # skip last layer of 5-layer neural net\n",
        "        break\n",
        "      # change tensor to numpy format, then set appropriate number of smallest weights to zero\n",
        "      layer_mask = torch.ones(len(layer.weight.reshape(-1)))\n",
        "      layer_copy = torch.flatten(layer.weight)\n",
        "      layer_copy = layer_copy.detach().numpy()\n",
        "      indices = abs(layer_copy).argsort() # get indices of smallest weights by absolute value\n",
        "      indices = indices[:int(len(indices)*prune_percentage)] # get k fraction of smallest indices\n",
        "      layer_mask[indices] = 0\n",
        "\n",
        "      # change masks of the layer\n",
        "      layer.mask = layer_mask.reshape(layer.weight.shape)\n",
        "\n",
        "\n",
        "  return sparse_m\n",
        "\n",
        "def train_iterative_prune(model, train_loader, epochs=10,learning_rate=0.001):\n",
        "  \"\"\"Function to train and prune  neural network \"\"\"\n",
        "\n",
        "  lossFunction = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  time0 = time()\n",
        "  total_samples = 0\n",
        "  t=0\n",
        "  prune_rates,prune_epochs = gen_prune_rate(max_prune_rate=0.98,start_epoch=0,num_prune_epochs=4,total_epoch=epochs)\n",
        "  print('Pruning rate: ',prune_rates,'Pruning epochs: ',prune_epochs)\n",
        "  for e in range(epochs):\n",
        "    print(\"Starting epoch\", e)\n",
        "    total_loss = 0\n",
        "    if e in prune_epochs:\n",
        "      model = prune_iterative(model,prune_rates[t])\n",
        "\n",
        "      acc = test(model, test_loader)\n",
        "      print(\"Pruning epoch : \",t,\"The accuracy after pruning is\", acc, \"%\")\n",
        "      t+=1\n",
        "\n",
        "    for idx, (images,labels) in enumerate(train_loader):\n",
        "      images = images.view(images.shape[0],-1) # flatten\n",
        "      optimizer.zero_grad() # forward pass\n",
        "      output = model(images)\n",
        "      loss = lossFunction(output,labels) # calculate loss\n",
        "      loss.backward() # backpropagate\n",
        "      optimizer.step() # update weights\n",
        "\n",
        "      total_samples += labels.size(0)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if idx % 100 == 0:\n",
        "        print(\"Running loss:\", total_loss)\n",
        "\n",
        "  final_time = (time()-time0)/60\n",
        "  if e == epochs-1 :\n",
        "    acc = test(model, test_loader)\n",
        "    print(\"Final accuracy  is\", acc, \"%\")\n",
        "  print(\"Model trained in \", final_time, \"minutes on \", total_samples, \"samples\")"
      ],
      "metadata": {
        "id": "HpIgDPacJ85O"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iterative_prune(masked_model, train_loader)"
      ],
      "metadata": {
        "id": "5F9YGsjHgUsp",
        "outputId": "6bc942c0-0918-4781-b9cd-376e2cef91a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruning rate:  [0.0, 0.5665625, 0.8574999999999999, 0.98] Pruning epochs:  [0, 2, 5, 8]\n",
            "Starting epoch 0\n",
            "Pruning epoch :  0 The accuracy after pruning is 96.28 %\n",
            "Running loss: 0.0650022104382515\n",
            "Running loss: 11.028556067496538\n",
            "Running loss: 20.92450230475515\n",
            "Running loss: 30.73391132429242\n",
            "Running loss: 41.84036269504577\n",
            "Running loss: 52.505147703923285\n",
            "Starting epoch 1\n",
            "Running loss: 0.27032271027565\n",
            "Running loss: 10.5007757358253\n",
            "Running loss: 19.731553816236556\n",
            "Running loss: 30.885079863481224\n",
            "Running loss: 41.837571321986616\n",
            "Running loss: 51.53740672208369\n",
            "Starting epoch 2\n",
            "Pruning epoch :  1 The accuracy after pruning is 96.46000000000001 %\n",
            "Running loss: 0.06796532869338989\n",
            "Running loss: 10.348097205162048\n",
            "Running loss: 20.757257852703333\n",
            "Running loss: 31.21351170912385\n",
            "Running loss: 41.735092628747225\n",
            "Running loss: 51.88886843435466\n",
            "Starting epoch 3\n",
            "Running loss: 0.16421878337860107\n",
            "Running loss: 10.584591135382652\n",
            "Running loss: 20.59803830459714\n",
            "Running loss: 31.016151901334524\n",
            "Running loss: 41.445236921310425\n",
            "Running loss: 51.7702876701951\n",
            "Starting epoch 4\n",
            "Running loss: 0.11790794134140015\n",
            "Running loss: 10.371203623712063\n",
            "Running loss: 20.36997719667852\n",
            "Running loss: 31.48727353103459\n",
            "Running loss: 41.922105852514505\n",
            "Running loss: 51.97772313468158\n",
            "Starting epoch 5\n",
            "Pruning epoch :  2 The accuracy after pruning is 92.0 %\n",
            "Running loss: 0.3476088345050812\n",
            "Running loss: 34.49665305018425\n",
            "Running loss: 67.52842013537884\n",
            "Running loss: 101.7749662399292\n",
            "Running loss: 136.596822604537\n",
            "Running loss: 170.01752346754074\n",
            "Starting epoch 6\n",
            "Running loss: 0.28615885972976685\n",
            "Running loss: 34.209548979997635\n",
            "Running loss: 68.34522092342377\n",
            "Running loss: 101.57627552747726\n",
            "Running loss: 135.11299109458923\n",
            "Running loss: 168.80828377604485\n",
            "Starting epoch 7\n",
            "Running loss: 0.2705279588699341\n",
            "Running loss: 34.26130169630051\n",
            "Running loss: 67.53700570762157\n",
            "Running loss: 101.28260436654091\n",
            "Running loss: 134.66412115097046\n",
            "Running loss: 169.59369285404682\n",
            "Starting epoch 8\n",
            "Pruning epoch :  3 The accuracy after pruning is 64.09 %\n",
            "Running loss: 2.188282012939453\n",
            "Running loss: 221.21362948417664\n",
            "Running loss: 440.1288561820984\n",
            "Running loss: 659.1591839790344\n",
            "Running loss: 878.092830657959\n",
            "Running loss: 1097.0409753322601\n",
            "Starting epoch 9\n",
            "Running loss: 2.192481279373169\n",
            "Running loss: 221.0870816707611\n",
            "Running loss: 440.0730531215668\n",
            "Running loss: 658.9686443805695\n",
            "Running loss: 877.8673830032349\n",
            "Running loss: 1097.013536453247\n",
            "Final accuracy  is 64.09 %\n",
            "Model trained in  5.022707990805308 minutes on  600000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNGO5Ih9Hv52"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Clearly, my hypothesis that accuracy will rise and then negatively correlate in a roughly linear way with pruning was incorrect. The figure instead shows a dramatic nonlinear relationship between accuracy and pruning. Accuracy remains roughly constant until dropping off at about 75% sparsity for weight-pruning and until 70% sparsity for unit-pruning. My hypothesis that unit-pruning impacts accuracy more dramatically than weight-pruning held up.\n",
        "\n",
        "These results are fascinating: Less than 25% of the neural net represents important information about its function. The data also suggest that accuracy may slightly increase with a light amount of pruning (~30%), although I would run on more iterations with a larger dataset to be sure. It would make sense that keeping the net's smaller weights reduces its generalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Consider the below statements about pruning and answer question Q1:\n",
        "\n",
        "A. In network pruning, we prune the small-weight connections: all connections with weights below a threshold are removed\n",
        "from the network.\n",
        "\n",
        "B. In network pruning, we prune the small-weight connections: all connections with weights above a threshold are removed\n",
        "from the network.\n",
        "\n",
        "C. Pruning is one of the methods for inference to efficiently produce models smaller in size, more memory-efficient, more power-efficient and faster at inference with minimal loss in accuracy, other such techniques being weight sharing and quantization.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q7fitFsxdfU4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHDYadgG-RHl",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. Which of the above statement(s) is/are true?\n",
        "Answer1 = \"Both A and C\" #@param [\"\",\"Both A and B\", \"Both B and C\", \"Both A and C\", \"Only A\", \"Only B\", \"Only C\"]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Consider the below statements about quantization and answer question Q2:\n",
        "\n",
        "A. Neural network quantization is a process of reducing the precision of the weights in the neural network, thus reducing the memory, computation, and energy bandwidths.\n",
        "\n",
        "B. When deploying neural networks models on mobile or edge devices, quantization and model compression in general, is desirable and often the only reasonable way to deploy a mobile model because the memory and computational budget of these devices is very limited.\n",
        "\n"
      ],
      "metadata": {
        "id": "-Dtz-Nlq1nz6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu3sufDD7pu1",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. Select the true statement(s) from the above:\n",
        "Answer2 = \"Both A and B\" #@param [\"\",\"Only A\",\"Only B\",\"Both A and B\",\"Neither A nor B\"]\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"na\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-",
        "outputId": "eac9da4e-202d-40a4-f27f-e227101eeb2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 2636\n",
            "Date of submission:  24 Feb 2024\n",
            "Time of submission:  15:50:54\n",
            "View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}