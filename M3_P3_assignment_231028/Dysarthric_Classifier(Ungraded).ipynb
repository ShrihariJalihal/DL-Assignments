{"cells":[{"cell_type":"markdown","metadata":{"id":"hH3UvBtnW755"},"source":["# Advanced Programme in Deep Learning (Foundations and Applications)\n","## A Program by IISc and TalentSprint\n","### Assignment: Speech and Audio Processing"],"id":"hH3UvBtnW755"},{"cell_type":"markdown","metadata":{"id":"KXubZhEt6g3u"},"source":["## Learning Objectives"],"id":"KXubZhEt6g3u"},{"cell_type":"markdown","metadata":{"id":"0shlrdB36iZs"},"source":["At the end of the experiment you will be able to :\n","\n","* extract the features from audio samples/data\n","* implement the Convolutional Neural Networks (CNN) model to classify emotions\n","* evaluate the CNN trained model on the testset"],"id":"0shlrdB36iZs"},{"cell_type":"markdown","metadata":{"id":"9iwLTAWXHKBS"},"source":["### Introduction\n","\n","Speech Dysarthria is a disorder in which speech muscles become weak, and it becomes difficult to articulate otherwise linguistically normal speech. This work is based on detection of speech dysarthria and how it can assist physicians, specialists, and doctors in its detection.\n","\n","TORGO Database of Dysarthric Articulation was developed by the University of Toronto's departments of Computer Science and Speech Language Pathology in collaboration with the Holland-Bloorview Kids Rehabilitation Hospital in Toronto, Canada. It contains approximately 23 hours of English speech data, accompanying transcripts and documentation from 8 speakers (5 males, 3 females) with cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS) and from 7 speakers (4 males, 3 females) from a non-dysarthric control group."],"id":"9iwLTAWXHKBS"},{"cell_type":"markdown","metadata":{"id":"VvNyZNEEJGYe"},"source":["### Dataset\n","\n","he TORGO database of dysarthric articulation consists of aligned acoustics and measured 3D articulatory features from speakers with either cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS), which are two of the most prevalent causes of speech disability (Kent and Rosen, 2004), and matchd controls. This database, called TORGO, is the result of a collaboration between the departments of Computer Science and Speech-Language Pathology at the University of Toronto and the Holland-Bloorview Kids Rehab hospital in Toronto.\n","\n","**Speakers:** Both CP and ALS result in dysarthria, which is caused by disruptions in the neuro-motor interface. These disruptions distort motor commands to the vocal articulators, resulting in atypical and relatively unintelligible speech in most cases (Kent, 2000). This unintelligibility can significantly diminish the use of traditional automatic speech recognition (ASR) software. The inability of modern ASR to effectively understand dysarthric speech is a major problem, since the more general physical disabilities often associated with the condition can make other forms of computer input, such as keyboards or touch screens, especially difficult (Hosom et al, 2003)."],"id":"VvNyZNEEJGYe"},{"cell_type":"markdown","metadata":{"id":"TQG8v5S2JS5O"},"source":["### Importing required packages"],"id":"TQG8v5S2JS5O"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ce621c0"},"outputs":[],"source":["import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import torch.nn as nn\n","from tqdm import tqdm\n","import librosa\n","from pathlib import Path\n","import torch.nn.functional as F\n","!pip install huggingface_hub\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"id":"7ce621c0"},{"cell_type":"markdown","metadata":{"id":"p4RTkSEyJXl2"},"source":["### Download the dataset torgo speech"],"id":"p4RTkSEyJXl2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2Mo-yBncIO1"},"outputs":[],"source":["from huggingface_hub import hf_hub_download\n","hf_hub_download(\n","                repo_id='viks66/torgo_speech',\n","                filename=\"torgo.zip\",\n","                cache_dir='./',\n","                force_filename='torgo.zip',\n","                repo_type='dataset',\n","                )"],"id":"h2Mo-yBncIO1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdcdf740"},"outputs":[],"source":["!unzip -q torgo.zip"],"id":"bdcdf740"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fd4815a"},"outputs":[],"source":["data_path = 'torgo/'"],"id":"8fd4815a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8df9a2fb"},"outputs":[],"source":["def get_files(path, extension='.wav'):\n","    return list(path.rglob(f'*{extension}'))"],"id":"8df9a2fb"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8c426554"},"outputs":[],"source":["all_files = get_files(Path(data_path))\n","all_files = [l for l in all_files if os.path.getsize(str(l)) != 0 ]\n","speakers = set([str(l).split('/')[-4] for l in all_files])\n","labels = {str(l):0 if 'C' in str(l).split('/')[-4] else 1 for l in all_files}\n","print(len(speakers), speakers)"],"id":"8c426554"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fffb7c68"},"outputs":[],"source":["test_speakers = ['F04', 'FC03', 'M05', 'MC04']"],"id":"fffb7c68"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1691f519"},"outputs":[],"source":["class DysarthricDataset(Dataset):\n","    def __init__(self, mode, test_speakers, labels ,num_val=200):\n","        if mode == 'train' or mode == 'val':\n","            label_names = sorted([l for l in labels if l.split('/')[-2] not in test_speakers])\n","        elif mode == 'test':\n","            label_names = sorted([l for l in labels if l.split('/')[-2] in test_speakers])\n","        if mode == 'val':\n","            label_names = label_names[:num_val]\n","        elif mode == 'train':\n","            label_names = label_names[num_val:]\n","        self.label_names = label_names\n","        self.label_dict = labels\n","\n","    def __len__(self):\n","        return len(self.label_names)\n","\n","    def __getitem__(self, idx):\n","        y, sr = librosa.load(self.label_names[idx])\n","        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13).T\n","        return torch.from_numpy(mfcc), self.label_dict[self.label_names[idx]]\n","\n","class BatchPadCollafeFn():\n","     def __init__(self):\n","        pass\n","     def __call__(self, batch):\n","        input_lengths, ids_sorted_decreasing = torch.sort(\n","            torch.LongTensor([len(x[0]) for x in batch]),\n","            dim=0, descending=True)\n","        max_input_len = input_lengths[0]\n","        mfcc_padded = torch.LongTensor(len(batch), max_input_len, batch[ids_sorted_decreasing[0]][0].shape[-1])\n","        mfcc_padded.zero_()\n","        labels = torch.LongTensor(len(batch))\n","        for i in range(len(ids_sorted_decreasing)):\n","            mfcc = batch[ids_sorted_decreasing[i]][0]\n","            mfcc_padded[i, :mfcc.shape[0], :] = mfcc\n","            labels[i] = batch[ids_sorted_decreasing[i]][1]\n","        return mfcc_padded, labels"],"id":"1691f519"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0d7aeb4"},"outputs":[],"source":["traindataset = DysarthricDataset(mode='train', test_speakers=test_speakers, labels=labels)\n","valdataset = DysarthricDataset(mode='val', test_speakers=test_speakers, labels=labels)\n","testdataset = DysarthricDataset(mode='test', test_speakers=test_speakers, labels=labels)\n","batch_size = 20\n","trainloader = DataLoader(traindataset, batch_size=batch_size, collate_fn=BatchPadCollafeFn())\n","valloader = DataLoader(valdataset, batch_size=batch_size, collate_fn=BatchPadCollafeFn())\n","testloader = DataLoader(testdataset, batch_size=batch_size, collate_fn=BatchPadCollafeFn())"],"id":"e0d7aeb4"},{"cell_type":"markdown","metadata":{"id":"ebakQDzzJfVp"},"source":["### Define the CNN model"],"id":"ebakQDzzJfVp"},{"cell_type":"code","execution_count":null,"metadata":{"id":"49859799"},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, in_channel=13):\n","        super().__init__()\n","        self.conv1 = nn.Conv1d(in_channel, 32, 3)\n","        self.conv2 = nn.Conv1d(32, 64, 3)\n","        self.conv3 = nn.Conv1d(64, 128, 3)\n","        self.dense = nn.Linear(128, 2)\n","\n","    def forward(self, x):\n","        x = x.permute(0, 2, 1)\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = torch.mean(x, -1)\n","\n","        return self.dense(x)"],"id":"49859799"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f51c6d13"},"outputs":[],"source":["def train(loader):\n","    model.train()\n","    n_classes = 2\n","    lossfn = nn.CrossEntropyLoss()\n","    confusion_matrix = torch.zeros(n_classes, n_classes)\n","    losses = []\n","    for data, label in tqdm(loader):\n","        data, label = data.to(device), label.to(device)\n","        out = model(data.float())\n","        loss = lossfn(out, label)\n","        optimiser.zero_grad()\n","        loss.backward()\n","        optimiser.step()\n","        losses.append(loss.item())\n","        _, preds = torch.max(out, 1)\n","        for t, p in zip(label.view(-1), preds.view(-1)):\n","            confusion_matrix[t.long(), p.long()] += 1\n","    return sum(losses)/len(losses), confusion_matrix.diag()/confusion_matrix.sum(1)\n","def val(loader):\n","    model.eval()\n","    n_classes = 2\n","    lossfn = nn.CrossEntropyLoss()\n","    confusion_matrix = torch.zeros(n_classes, n_classes)\n","    losses = []\n","    for data, label in tqdm(loader):\n","        data, label = data.to(device), label.to(device)\n","        out = model(data.float())\n","        loss = lossfn(out, label)\n","        losses.append(loss.item())\n","        _, preds = torch.max(out, 1)\n","        for t, p in zip(label.view(-1), preds.view(-1)):\n","            confusion_matrix[t.long(), p.long()] += 1\n","    return sum(losses)/len(losses), confusion_matrix.diag()/confusion_matrix.sum(1)"],"id":"f51c6d13"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1cacfdd"},"outputs":[],"source":["device = 'cuda'\n","lr = 0.0001\n","model = Model().to(device).float()\n","optimiser = torch.optim.Adam(model.parameters(), lr=lr)"],"id":"a1cacfdd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b45f0750"},"outputs":[],"source":["num_epochs = 10\n","trainloss, trainaccs, valloss, valaccs = [], [], [], []\n","for ep in range(num_epochs):\n","    loss, accs = train(trainloader)\n","    trainloss.append(loss)\n","    trainaccs.append(accs)\n","    loss, accs = val(valloader)\n","    valloss.append(loss)\n","    valaccs.append(accs)\n","    print(trainloss[-1], valloss[-1])\n","    print(trainaccs[-1], valaccs[-1])"],"id":"b45f0750"}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}