{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Advanced Programme in Deep Learning (Foundations and Applications)\n","## A Program by IISc and TalentSprint\n"],"metadata":{"id":"HcCpUVoBdwKi"}},{"cell_type":"markdown","metadata":{"id":"doENGez0k03C"},"source":["## Learning Objectives"]},{"cell_type":"markdown","source":["* understand the different regularization methods to avoid the overfitting of neural networks"],"metadata":{"id":"8-E2_mU8eFhe"}},{"cell_type":"markdown","metadata":{"id":"9-sRjaldCcCI"},"source":["## Dataset\n","\n","\n","The original MNIST dataset contains handwritten digits. People from AI/ML or Data Science community love this dataset. They use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset they would try on. As per popular belief, If the algorithm doesn’t work on MNIST, it won’t work at all. Well, if algorithm works on MNIST, it may still fail on other datasets.\n","\n","\n","As per the original [paper](https://arxiv.org/abs/1708.07747) describing about Fashion-MNIST, It is a dataset recomposed from the product pictures of Zalando’s websites. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits.\n","\n","There are some good reasons for the challenges faced by MNIST dataset:\n","\n","* MNIST is too easy - Neural networks can achieve 99.7% on MNIST easily, and similarly, even classic ML algorithms can achieve 97%.\n","\n","* MNIST is overused - Almost everyone who has experience with deep learning has come across MNIST at least once.\n","\n","* MNIST cannot represent modern CV task\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ijqHaydutZ5K"},"source":["### Description"]},{"cell_type":"markdown","metadata":{"id":"6ANHcoEyg_gp"},"source":["The dataset choosen for this experiment is Fashion-MNIST. The dataset is made up of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images.\n","\n","Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.\n","\n","**Labels / Classes**\n","\n","0 - T-shirt/top\n","\n","1 - Trouser\n","\n","2 - Pullover\n","\n","3 - Dress\n","\n","4 - Coat\n","\n","5 - Sandal\n","\n","6 - Shirt\n","\n","7 - Sneaker\n","\n","8 - Bag\n","\n","9 - Ankle boot"]},{"cell_type":"markdown","source":["### Import required packages"],"metadata":{"id":"nWxKxA5yPxor"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.datasets import fashion_mnist\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Dropout\n","from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt"],"metadata":{"id":"8gDIwA91PwLc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load and preprocess Fashion MNIST dataset\n","(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n","X_train, X_test = X_train / 255.0, X_test / 255.0"],"metadata":{"id":"fnFX_fQlP_k9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**X_train / 255.0:** This operation divides every pixel value in the training images (X_train) by 255.0. Since the division is element-wise, each pixel value is scaled down to a value between 0 and 1.\n","\n","**X_test / 255.0:** Similarly, this operation divides every pixel value in the test images (X_test) by 255.0.\n","\n","After this normalization step, the pixel values of the images are in the range `[0, 1]`, which is a common range for input data to neural networks. This can help improve the convergence of the optimization process and the overall training performance of the model."],"metadata":{"id":"z_MCwCDwQXhg"}},{"cell_type":"code","source":["# Dataset class names\n","class_names = [\n","    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n","]\n","\n","# Visualize a few images with their class labels\n","plt.figure(figsize=(10, 10))\n","for i in range(25):\n","    plt.subplot(5, 5, i + 1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(X_train[i], cmap=plt.cm.binary)\n","    plt.xlabel(class_names[y_train[i]])\n","plt.show()"],"metadata":{"id":"KgiSxwf1QKau"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Intentionally creating an overfitting scenario is a useful way to demonstrate the effectiveness of regularization techniques in deep neural networks. Let's break down why an overfitting scenario is important when showcasing these techniques:\n","\n","**1. Understanding Overfitting:**\n","\n","Overfitting occurs when a machine learning model performs well on the training data but poorly on unseen test data. It's a result of the model learning to capture noise or specific details in the training data that don't generalize to other data. By creating an overfitting scenario, you can showcase how models can become excessively complex and tailored to the training data.\n","\n","**2. Need for Regularization:**\n","\n","Regularization techniques help prevent or mitigate overfitting by constraining the complexity of the model. These techniques encourage the model to generalize well to new data rather than memorizing the training data. Demonstrating regularization techniques in an overfitting scenario highlights their role in improving model generalization.\n","\n","**3. Visualizing the Problem:**\n","\n","Visualizing an overfitting scenario can provide a clear visual representation of the problem. When you see training loss decreasing while validation loss starts increasing, it's a sign of overfitting. Visualizing this scenario helps you understand why regularization is necessary.\n","\n","**4. Comparing Techniques:**\n","\n","By applying regularization techniques to an overfitting model, you can directly compare the impact of these techniques on model performance. You can observe how each technique modifies the training and validation loss curves, showing how they prevent or reduce overfitting."],"metadata":{"id":"zlzY6b5wQ2Ml"}},{"cell_type":"code","source":["# Create an overfitting scenario by using a small dataset\n","small_X_train, small_y_train = X_train[:1000], y_train[:1000]"],"metadata":{"id":"Yq-UqZNBQnCB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build a deep neural network model (intentionally prone to overfitting)\n","model_overfit = Sequential([\n","    Flatten(input_shape=(28, 28)),\n","    Dense(256, activation='relu'),\n","    Dense(128, activation='relu'),\n","    Dense(10, activation='softmax')\n","])"],"metadata":{"id":"BF3l9pZsWTLJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# summary of the architecture of the neural network model\n","model_overfit.summary()"],"metadata":{"id":"t8D1-pazXk3C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_overfit.compile(optimizer='adam',\n","                     loss='sparse_categorical_crossentropy',\n","                     metrics=['accuracy'])"],"metadata":{"id":"yfiO8bSsXIw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the overfitting model\n","history = model_overfit.fit(small_X_train, small_y_train, epochs=50, validation_split=0.2)"],"metadata":{"id":"8qjY9nPaXMb2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Access training and validation loss\n","training_loss = history.history['loss']\n","validation_loss = history.history['val_loss']"],"metadata":{"id":"L8ut1kB6YGb4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Access training and validation accuracy (if applicable)\n","training_accuracy = history.history['accuracy']\n","validation_accuracy = history.history['val_accuracy']\n","\n","# Print the final training and validation loss\n","final_training_loss = training_loss[-1]\n","final_validation_loss = validation_loss[-1]\n","print(f\"Final Training Loss: {final_training_loss:.4f}\")\n","print(f\"Final Validation Loss: {final_validation_loss:.4f}\")\n","\n","# Print the final training and validation accuracy (if applicable)\n","if training_accuracy and validation_accuracy:\n","    final_training_accuracy = training_accuracy[-1]\n","    final_validation_accuracy = validation_accuracy[-1]\n","    print(f\"Final Training Accuracy: {final_training_accuracy:.4f}\")\n","    print(f\"Final Validation Accuracy: {final_validation_accuracy:.4f}\")"],"metadata":{"id":"cVsOr5pTYO_-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The **model_overfit.fit()** function trains the model_overfit neural network using the provided training data (small_X_train and small_y_train). The training process involves iterating through the training dataset for a specified number of epochs, with a subset of the training data reserved for validation. The history variable stores information about the training process, which can be used for further analysis and visualization."],"metadata":{"id":"AU5Z7aa9XdDS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdQAipgpKZaX"},"outputs":[],"source":["# Plot training and validation loss\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='validation')\n","plt.title('Overfitting Scenario: Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","source":["Now, let's introduce regularization techniques to overcome overfitting:"],"metadata":{"id":"E011G5Y9Y03g"}},{"cell_type":"markdown","metadata":{"id":"VvBrjDXmdqOX"},"source":["###  Optimizers"]},{"cell_type":"markdown","metadata":{"id":"4Tv15JM0dqOX"},"source":["Some popular optimizers used for boosting the speed in training large deep neural networks are: Momentum optimization, RMSProp, and Adam optimization. Refer [here](https://mlfromscratch.com/optimizers-explained/#/) for a detailed understanding."]},{"cell_type":"markdown","metadata":{"id":"uA4iM1aQdqOY"},"source":["#### Momentum Optimization"]},{"cell_type":"markdown","metadata":{"id":"Yz-Y0M-SdqOZ"},"source":["Momentum  optimization subtracts  the  local  gradient  from  the  momentum  vector  m  (multiplied  by  the  learning  rate  η),  and  it  updates  the  weights  by  simply  adding  this momentum vector, thus accelerating the speed. The momentum hyperparameter $β$ is introduced to prevent  the momentum from growing too large (set between 0 and 1, typically 0.9).\n","\n"]},{"cell_type":"markdown","source":["### 1. Momentum Optimizer:\n"],"metadata":{"id":"cx-oQK1AZr5r"}},{"cell_type":"markdown","metadata":{"id":"cFp9_IkfdqOb"},"source":["#### RMSProp"]},{"cell_type":"markdown","metadata":{"id":"1cWLQD2hdqOc"},"source":["The RMSProp algorithm fixes only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step.\n","\n","The decay rate $β$ is typically set to 0.9. Yes, it is once again a new hyperparameter, but this default value often works well, so we may not need to tune it at all.\n"]},{"cell_type":"markdown","metadata":{"id":"v1vPNNs7dqOe"},"source":["#### Adam Optimization"]},{"cell_type":"markdown","metadata":{"id":"wMvwuAridqOf"},"source":["Adam combines the ideas of Momentum  optimization  and  RMSProp:  it keeps track of both, an  exponentially  decaying  average  of  past  gradients,  and  an  exponentially  decaying  average  of  past  squared  gradients.\n","\n","The momentum decay hyperparameter $β_1$ is typically initialized to 0.9, while the scaling  decay  hyperparameter  $β_2$  is  often  initialized  to  0.999."]},{"cell_type":"markdown","source":["### 2. Dropout:\n","\n","Add dropout layers after each hidden layer:"],"metadata":{"id":"_J0d-BXJZB18"}},{"cell_type":"markdown","metadata":{"id":"23QlGnLbdqO2"},"source":["Dropout  is  one  of  the  most  popular  regularization  techniques  for  deep  neural  networks. At each training stage, individual nodes are either dropped out of the net with probability 1-p or kept with probability p, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed.\n","\n","![Image](https://i.ibb.co/HnfSTyX/M5-2.jpg)\n","\n","$\\text{Figure: Dropout Regularization}$\n","\n","To  implement  dropout  using  Keras,  we  can  use  the  keras.layers.Dropout  layer. During  training,  it  randomly  drops  some  inputs  (setting  them  to  0)  and  divides  the remaining inputs by the keep probability. After training, it just passes  the  inputs  to  the  next  layer.  For  example,  the  following  code  applies  dropout regularization before every Dense layer, using a dropout rate of 0.5:"]},{"cell_type":"markdown","source":["### 3. L2 Regularization:\n","\n","Apply L2 regularization to all hidden layers:"],"metadata":{"id":"_9fpfjprZO-_"}},{"cell_type":"markdown","metadata":{"id":"pTtpElDodqOv"},"source":["Deep neural networks may have millions of parameters. The network, therefore,   has vast freedom and can fit a huge variety of complex datasets. This flexibility however also makes it prone to overfitting the training set. Thus we need regularization.\n","\n","Let us now see some popular regularization techniques for neural networks: $ℓ1$ and $ℓ2$ regularization and dropout"]},{"cell_type":"markdown","metadata":{"id":"Y7-pZLbsdqOw"},"source":["We can use $ℓ1$ and $ℓ2$ regularization  to  constrain  a  neural  network’s  connection  weights  (but  typically  not  its  biases).  Here  is  how  to  apply  $ℓ2$  regularization  to  a  Keras  layer’s  connection  weights, using a regularization factor of 0.001:"]},{"cell_type":"markdown","source":["### 4. Early Stopping:\n","\n","Add early stopping callback to stop training if validation loss doesn't improve:"],"metadata":{"id":"sZr3TAP9ZJG3"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.callbacks import EarlyStopping"],"metadata":{"id":"wE0rFkva8DYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build a model with regularization techniques\n","regularized_model = Sequential([\n","    Flatten(input_shape=(28, 28)),\n","    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),  # L2 regularization\n","    Dropout(0.5),  # Dropout layer\n","    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n","    Dropout(0.5),\n","    Dense(10, activation='softmax')\n","])"],"metadata":{"id":"RyruIaYH7hHn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compile the regularized model\n","regularized_model.compile(optimizer='adam',\n","                         loss='sparse_categorical_crossentropy',\n","                         metrics=['accuracy'])\n","\n","# Define Early Stopping callback\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"],"metadata":{"id":"tCBTbFnO7x9Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the regularized model\n","history_regularized = regularized_model.fit(X_train, y_train, epochs=50,\n","                                            validation_data=(X_test, y_test),\n","                                            callbacks=[early_stopping])"],"metadata":{"id":"DVSgSzKR702q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualize the loss after applying regularization"],"metadata":{"id":"a9WQ5eZtchRG"}},{"cell_type":"code","source":["# Access training and validation loss\n","training_loss = history_regularized.history['loss']\n","validation_loss = history_regularized.history['val_loss']\n","\n","# Access training and validation accuracy (if applicable)\n","training_accuracy = history_regularized.history['accuracy']\n","validation_accuracy = history_regularized.history['val_accuracy']\n","\n","# Print the final training and validation loss\n","final_training_loss = training_loss[-1]\n","final_validation_loss = validation_loss[-1]\n","print(f\"Final Training Loss: {final_training_loss:.4f}\")\n","print(f\"Final Validation Loss: {final_validation_loss:.4f}\")\n","\n","# Print the final training and validation accuracy (if applicable)\n","if training_accuracy and validation_accuracy:\n","    final_training_accuracy = training_accuracy[-1]\n","    final_validation_accuracy = validation_accuracy[-1]\n","    print(f\"Final Training Accuracy: {final_training_accuracy:.4f}\")\n","    print(f\"Final Validation Accuracy: {final_validation_accuracy:.4f}\")"],"metadata":{"id":"_eLUmOylbR27"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot training and validation loss for the regularized model\n","plt.plot(history_regularized.history['loss'], label='Train Loss')\n","plt.plot(history_regularized.history['val_loss'], label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.title('Regularized Model')\n","plt.show()"],"metadata":{"id":"ebftT15kbrGV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The above showed regularization techniques are just help you to understand the basic idea of how to use these method in overfitting scenarios you can also tweak the hyperparameters to reduce the overfitting and to see the better performance.\n","\n"],"metadata":{"id":"tXGZjibtc0ao"}}]}